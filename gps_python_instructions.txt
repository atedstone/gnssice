========================================================
University of Edinburgh Glaciology Group
DGPS Processing with Python
========================================================

Andrew Tedstone 2012-14, based on previous documentation by Andrew Sole and Ian Bartholomew, plus workflow approach by Matt King (Newcastle).

 
IMPORTANT!
-----------
Every time you start a new session (i.e. PuTTY terminal window), run:

	module load applications/gamit/10.4

Which will give access to TEQC/GLOBK/Gamit/Track.

Also make sure to start your PuTTY session with XMing running.


Summary of workflow / manual contents
-------------------------------------
1. Ensure everything is installed.
2. Download orbit (sp3) files for year.
3. Copy base and rover leica files to working directory.
4. Download kellyville data to cover known gaps in our base record.
5. Convert leica files to windowed rinex files (base and rover).
6. Do track kinematic processing.
    6a. Process 'temporary' fixes taken during redrilling/flights.
7. Concatenate daily track geod files to year.
    7a. Correct for pole leans etc, if possible.
8. Convert North-East-Up coordinates to velocities (currently a matlab script).

 

Getting started
---------------
This setup works on burn.geos.ed.ac.uk, which at the time of writing has TEQC/GLOBK/Gamit/Track, and Python 2.6.
See https://www.geos.ed.ac.uk/it/FAQ/gamit.html.

You should check that the GAMIT tables are sufficiently up to date for your captured GPS epochs. If they are not, request that IT update them before commencing processing. (Check the track website, below, for updates.)

More about TEQC:
http://facility.unavco.org/software/teqc/
More about GLOBK/Gamit:
http://chandler.mit.edu/~simon/gtgk/script_help.htm
More about Track:
http://geoweb.mit.edu/~tah/track_example/

If you haven't already got it, clone the pygps project from Andrew Tedstone's bitbucket (atedstone) to your networked storage e.g. /home/<USER>/pyscripts/ (e.g. via SSH protocol):
	git clone ssh://git@bitbucket.org/atedstone/pygps.git
The project needs to be put on your python path:
 - Make sure you can see hidden files and folders.
 - Open /home/<USER>/.bashrc for editing.
 - Add the following (e.g.):
		PYTHONPATH="${PYTHONPATH}:/home/<USER>/pygps/"
		export PYTHONPATH
 - Save and close.
This means that it won't matter where you then do your main working - Python will automatically find the scripts. 

Ensure that gps.py, process_rinex.py and process_dgps.py are executable - e.g.:
	chmod u+x <file_name>

Set up a working directory in your scratch space, e.g. /scratch/<USER>/gps_workspace_<YEAR>/.
 - You'll probably want to clear up as you go, otherwise you'll end up with loads of files, e.g. once the leica files have been converted to rinex, delete the leica files, and so on. The scripts purposefully do not try to clean up.
 
In this working directory, clone the GPS configuration files, ensuring that the files land up in a subdirectory called gps_config, i.e. (via SSH protocol):

	git clone ssh://git@bitbucket.org/atedstone/lev_gps_config.git gps_config

(Alternatively, if setting up processing for a new field campaign in a different location, you may wish to clone this project elsewhere and use the files as templates for your own parameter files to create your own gps_config folder, which you can then version-control if you wish.)
	
pygps will then look for track cmd files and rover-specific configuration files in this location. If you modify these files, particularly the rover-specific ones, be sure to commit your changes back to the repository once you are done.

Files from kellyville are sometimes in the compressed rinex format.
If they are, you will also need RNXCMP, available at:
 http://terras.gsi.go.jp/ja/crx2rnx.html
You need to install this to your home directory.
At the time of writing, fleet requires the Linux 32 bit version.
 1. Download the tar.gz file.
 2. uncompress the download:
		tar -zxvf <filename>
 3. Put the files on your unix path. Either:
     a. Copy the files from <RNXCMPDirectory>/bin/ directly into /home/<USER>/bin/ 
	 -or-
	 b. Add the <RNXCMPdirectory>/bin/ to your unix path (you'll need to find out how to do this).

	 
Note on gps.py functionality
----------------------------
This file contains the three main classes used for Python-based GPS processing:
	gps.RinexConvert
	   .Kinematic
	   .PostProcess
To get full information on the module, at the command line run:
	pydoc gps   (this displays documentation on the command line)
	 -or-
	pydoc -w gps   (this saves documentation to gps.html, to be viewed in a web browser)
	   
In general the functions have been designed to be non-interactive. They can therefore be used in automated batch processing. Only gps.Kinematic.track must be run interactively.

Some of the provided interface scripts to the classes are interactive (e.g. process_rinex, process_dgps) but these could be revised for automated batch processing as they simply ask the user to input the arguments that are then passed on to the classes.

RinexConvert has been developed with Leica 530 and 1200 receivers in mind. Some functionality works with other receivers, e.g. window_overlap works with Trimble dat files (but not .T00/.T01).

Some critical functions have been enabled to take advantage of python's command line functionality, i.e.
	python -m gps <function name> <arguments>
To find out what's available in this way, just do:
	python -m gps
See also http://docs.python.org/using/cmdline.html for more information.


To quit processing....
----------------------
Press CTRL + C. You might then need to press Enter. 

If a Python script then the screen will show "KeyboardInterrupt", and may also show a bunch of traceback info - don't worry about this.


File naming conventions
-----------------------
Orbit files: igs<doy>.sp3
Rinex files:
	If compressed: *.<yy>d
	If uncompressed: *.<yy>o
	Daily files: <site>_<doy>0_d.<yy>o
	Overlapped/windowed files: <site>_<doy>0_ol.<yy>o
	File with multiple days: all_<site>.<yy>o
Files generated by track:
	Log file: <rover>_<base>_<doy>.out
	Results file: <rover>_<base>_<doy>GEOD.dat
				  <rover>_<base>_<doy>NEU.dat
    Figures: trackpy.NEU.<rover>.LC.<doy>.eps
Track processing session command file: gps.track.<rover>.log	
Track command files: track_<base>.cmd
Rover config files: config_<rover>.xml
Post-processed files: <rover>_<year>_geod.dat -or- 
					  <rover>_<start year>_<end year>_geod.dat
	


Get the GPS files together
--------------------------
Make sure you are putting all these following files into the scratch/working space you set up above.

Use a terminal window (e.g. PuTTY) to do the following....

1. Get the orbit files:

	 gps.py get_orbits <year> <start doy> <end doy>
	 
   N.b. don't attempt this over a change in year, e.g. start of 360 and end of 4. Instead do two calls, e.g.
	 gps.py get_orbits 2011 360 365
	 gps.py get_orbits 2012 1 4
   The only problem will then be that days 365 and 1 don't contain the next and previous days data respectively.
   Just copy and paste from the relevant files into the next, or on unix command line use cat, e.g:
		cat igs364.sp3 igs365.sp3 igs001.sp3 > igs365.sp3
   Also remember that the .sp3 naming scheme only contains the day of year as this is what Track understands -
   so if you're not careful when downloading from > 1 year you'll end up overwriting files.
   ---> Best to only deal with one year's worth of data in scratch space at a time.
   
2. Copy the raw leica files for the rover and base into the scratch space.
	- Copying all rovers over in one go isn't a problem if you want to do that - but possibly will add to your confusion!
	
3. If required, download rinex files for kellyville to cover the gaps:   
 a. Download: 
		sh_get_rinex -archive sopac -yr 2011 -doy 0 -ndays 250 -sites kely
	If files have *.<yy>o suffix you're all set, otherwise...	
 b. If they are zipped, unzip the compressed rinex files using 7zip or whatever
 c. Then convert to normal rinex i.e. from *.10d to *.10o:
	    gps.py crx2rnx <suffix, e.g. 10d>
 d. Overlap/window the kellyville rinex files:
      See 'Convert leica files to Rinex', but choose appropriate option to deal with rinex files at command line.
		
4. Check the obs_file file extensions in the track cmd files (in gps_config) - they need to be set to the correct year suffix, otherwise you'll get 'olding' errors from track.
		
	
	
Convert leica files to Rinex
----------------------------
We convert to rinex files which are windowed to 28hrs duration: from 22:00 the previous day to 02:00 the next day.

In your terminal window, make sure you're in your scratch gps directory.

There's two possibilities, depending on whether the data were saved to daily files or one big lump.
Both are addressed by one script...
Run this command:

	process_rinex.py
	
Follow on-screen instructions. Be aware that if you're processing from one big leica file you'll be asked to specify the start and end dates. The processing will then commence and will tell you what it is doing. You can wander off, it shouldn't need your attention.

Make sure you create rinex files for each site (i.e. run the script for each site).



Do the kinematic processing
---------------------------

Pre-requisites:
	- Firstly, make sure you have appropriate .cmd files for your base station site. These are already available for the leverett transect (track_levb.cmd and track_kely.cmd, in lev_gps_config). My initial recommendation for shorter baselines with 10-15sec sampling would be to use track_levb.cmd as a template. For longer baselines with sparser sampling, use track_kely.cmd as a template.
	- Secondly, open pygps/process_dgps.py and ensure that the default parameters for processing with your base station are set up - follow the format in the file. Again, set initial values based on my recommendation above, unless you've more specific information to go on... 

Open a terminal window at the scratch location. 
***If using PuTTY make sure you also have Xming working - for figure windows.***

Run:
	
	process_dgps.py
	
You don't have to process an entire site in one session. Enter the start day and guesstimate an appropriate end day. If you get fed up before the end day is reached, do CTRL+C to break out/halt the process (preferably between processing two days of data, rather than during).

This script (process_dgps.py) has defaults for ion_stats, MW_WL and LG set. These vary depending on the specified base station: at the time of writing, levb and kely are both supported.

For first day for each site use a priori coordinates of each site derived from online 
 http://apps.gdgps.net/apps_file_upload.php (lev_apr_coords.txt)
 Also, webapp.geod.nrcan.gc.ca/geod/tools-outils/ppp.php
 - upload a rinex file and specify the day from which to return a priori coordinates.
Subsequently select no. The program takes the APR coordinates from the previous days results.

In 2012 I had problems with coordinates from the above website. However, teqc estimates the daily positions and puts them in the top of daily rinex files - these seem to do the trick, so I used these instead. You might also find that the site_stats (see later) initially have to be loosened to deal with the APR coordinates. (AJT, September 2012).

A number of additional arguments are hard-set in the cmd files created for each base station, (e.g. levb, kely), e.g. site_stats and bf_set. These probably don't need to be modified from their default values...

By default, track is set up to accept the day's results automatically if the Spearman coefficient between East and North exceeds the specified threshold. If the day fails the test, the script will ask you what action to take next.

You can change the value of the specified threshold at runtime by setting it in process_dgps.py as follows:
	k.track_spearman_threshold = 0.66

Track can also be set up to require the user to accept every day manually - provide use_auto_qa=False as an argument to the call to gps.Kinematic.track in process_dgps. 

Initially, track will try to use the default ion_stats, MW_WL and LG parameters as specified in process_dgps.py.

If you choose to reject track's initial results based on the default parameters, you can enter your own:

	- ion_stats 
		This parameter sets ion_stats in the track cmd file. It seems to do something - not sure what - even though the track documentation suggests that many more arguments for ion_stats are required. 	Does it just set <jump>? (String number: <S06>)
		
	- MW_WL weighting 
		Weight to be given to deviation of MW-WL from zero. Default is 1 (ie., equal weight with LC residuals). Setting the value smaller will downweight the contribution of the MW-WL. For noisy or systematic range data (can be tested with a P1,P2 or PC solution), the WL_fact may be reduced.
		MW WL stands for Melbourne-Wubbena (MW) wide lane (this is the combination of range and phase that 
		gives the difference between the L1 and L2 biases and it independent of the ionospheric delay and 
		the geometry - track help file line 266). The parameter is taken by the float_type command, setting @FLOAT_TYPE <WL_Fact> (string number: <S04>).

	- LG combination weighting 
		Weight to be given to deviation of the Ionospheric delay from zero.  Default is 1 (i.e., ionospheric delay is assumed to be zero and given unit weighting in deterimining how well a set of integer ambiquities fit the data.  On long baselines, value should be reduced.
		For long baselines (>20 km) the <Ion_fact> should be reduced to give less weight to the ionospheric delay constraint.  For 100 km baselines, 0.1 seems to work well.  With very good range data (ie., WL ambiquities all near integer values), this factor can be reduced. Sets <Ion_fact> argument for @FLOAT_TYPE (string number <S05>).
		
The results should eventually pop up in a figure window, and the RMS values should appear in the terminal window. Particularly if using kely, processing times of >15 mins per day are not unusual.

The figure window will block input to the terminal window until it is closed. Unfortunately there's no way around this with the version of python currently on burn.

You'll be asked to input some information for logging - a quality identifier, and an optional longer comment. These are appended the the logging file, named gps.track.<site>.log.
Quality identifiers:
G: Good
O: Ok
B: Bad
A: Accepted Automatically (by Spearman test)

If RMS high and results not good, try (in the following order)
	- Increase ion_stats, up to maybe 3 or 4. Choose the combination that gives the lowest rms and produces the closest to a straight line when plotted.
	- Reduce MW_WL weighting from default=1 to say 0.5 if data appear noisy.
	- Possibly try reducing LG weighting - especially for sites further away where the ionosphere has an increasingly large effect.

We used to suggest that site_stats could be increased if results looked to be of poor quality. However, now we think we understand this more... the defaults set in the cmd file reflect our predictions that sites are unlikely to be > 10 metres away from a priori position (ish), and should not have moved more than 2 metres compared to the last epoch. So, increasing these values shouldn't really help... (but it does sometimes!)

Possible errors:
 - SP3 Interpolation Errors. This may mean that there are problems with one or more satellites in the SP3 orbit files. Open the .out file for the day which just failed and go to the bottom of it. Check the line(s) which say "Interpolation error PRN" - the number directly after this is the satellite. Open the cmd file for the relevant base station and add:
  exclude_svs satellite_number
E.g.:
  exclude_svs 21
Save the file and close it, then re-run the day. 

- (e.g.) Track IOSTAT error:  IOSTAT error      2 occurred opening gps_config/track_kely.cmd in MAKEXK - this probably means you're trying to process while not in your working directory.

- !!APR .LC file does not exist! Terminating processing. If you are expecting a .LC file to exist this may again mean that you are trying to process while not in your working directory.

- STOP FATAL Error: Stop from report_stat. Have you remembered to update the RINEX file suffix in the track CMD files to reflect the year you are processing?

- Something about fortran files: you probably haven't loaded gamit module - see start of this doc.



--------Old ideas on trying to improve results - sometimes work but we're even less certain why!
	- increase site_stats in the .cmd (track_kely.cmd/track_levb.cmd) file (df=10 10 10 1 1 1, try 100 100 100 10 10 10 first, then gradually increase)
	- increase bf_set in the .cmd file (bf= 2 40, try 5 100 and then increase gradually)
	- increase or decrease ion_stats in the input parameters (df=1, try 0.3 first then 2,3 if that dosnt work)
	
	
If a day or more accidentally or otherwise get missed during processing and you need to go back to re-process them:
- Find the NEU and GEOD files for the day prior that on which you need to re-start processing.
- Make sure they are in your workspace directory.
- Rename to track.NEU.<site>.LC and track.GEOD.<site>.LC
- This ensures that track uses the previous day's coordinates for a priori estimate positioning.
- If you haven't finished processing the rest of the season, be sure to change filenames back to the appropriate LC files.
	

Concatenate the track files
---------------------------
The GEOD track output files need to be combined together into one big file, removing the overlapping hours.

Use the gps.PostProcess class for this.

Prepare a site-specific XML configuration file, config_<site>.xml and save it in <working>/gps_config/.
For Leverett processing these should already be available, listing configurations for years of observation  already undertaken. Get them from the glaciology repository (see Getting Started).


Example format of XML configuration file:

<config rover="<rover>">

	<!-- This element lists all the datasets to concatenate. -->
	<concatenate>
		<!-- Have a conc element for every dataset to concatenate. 
		CHRONOLOGICAL ORDER, oldest to newest. -->
		<conc year="2011" base="levb" extra_id=""> <!-- see below for info on extra_id -->
			<!-- These are the days of year to exclude - you can specify ranges of dates. -->
			<ex from="165" to="168" />
			<ex from="170" to="170" />
		</conc>
		
		<conc year="2012" base="levb" extra_id="">
		<!-- This year doesn't have any days to exclude. -->
		</conc>
	</concatenate>

	<!-- Here list any corrections to be made to the north-east-up values.
		Correction will be made beyond and including the specified fractional doy.
		Positive values will be added, negative values subtracted.
		 CHRONOLOGICAL ORDER, oldest to newest. -->
	<correct>
		<corr year="2011" doy="180.45" n="1.09" e="0.45" u="0.28" />
		<corr year="2012" doy="190.45" n="0.09" e="0.25" u="0.29" />
	</correct>

</config>	

If you want to exclude a particular dataset or correction from post-processing, enclose it in comments, i.e. <!-- <element> -->

CURRENT STRATEGY - AUTUMN 2012:
New data are appended to the existing - large - GEOD dat files which run from 2009 onwards. These GEOD dat files are already corrected and the data within them therefore do not need further processing.

This means that datasets contained within the GEOD file do not need to be listed within <concatenate>. Indeed, at present, they cannot be, as the code which skips datasets already present in the GEOD file is disabled.

So, just list the new datasets to add to the existing GEOD dat files in <concatenate>.

You can leave <corr /> lines from previous years in - these WILL be skipped over if they occur within the time bounds of the GEOD dat file. 

If you are processing over the change in year, e.g. 2013 --> 2013, you'll probably have files from both years in your working directory. This will confuse the processing script, so you need to do a bit of manual concatentation first.
>> ipython
>> import gps
>> pp = gps.PostProcess()

Now run this function for each 'dataset' you need to process:
>> pp.concatenate_daily_GEOD('base','rover',year,start_doy=start,end_doy=end,save_to_file=True)

E.g.:
>> pp.concatenate_daily_GEOD('levb','lev8',2013,start_doy=1,end_doy=132,save_to_file=True)
>> pp.concatenate_daily_GEOD('levb','lev8',2012,start_doy=242,end_doy=276,save_to_file=True)
>> pp.concatenate_daily_GEOD('kely','lev8',2012,start_doy=277,end_doy=366,save_to_file=True)

By doing this, you create files associated with each base, which the script will find when it looks through the <concatenate> list.

Now, run:
		gps.py concatenate_geod <rover>
		
The script will find the configuration file. Basically you'll first concatenate just this years data. Check the plots which result. Use this script iteratively to work out which corrections to add to the config file.

Once happy with the corrections, you can add this years data to the existing GEOD dat file. Run:
  gps.py concatenate_geod <rover> <path and filename of GEOD dat file>

N.b. if processing spring data, where re-drills are right at the end of last year's already processed data, it might make sense to skip running on just the new data, and include the existing data immediately instead.
  
 
OTHER STUFF ON THIS SCRIPT (pretty much says the same as above):
The script will also look for existing concatenated GEOD files (which you might have produced using PostProcess.concatenate_daily_GEOD), named as: <base>_<rover>_<year>_<extra_id>_geod.dat. This is the only thing for which extra_id is used - as an extra specifier in the filename of existing concatenated files. It is OPTIONAL.
                              
You may optionally provide the filename of a file which already contains concatenated data from multiple years. This file must already have N-E-U columns and have had all required corrections applied. The Fract DOY column must have been appropriately incremented to account for years. The Fract DOY of the last record will be used as the starting date for concatenating data. If the constituent datasets do not already have ID numbers then all rows from the file will be given one bulk ID.

N.b. if you have corrections to make you'll probably have to run this script at least once without corrections in order to use the plots to work out the corrections to apply. It might be useful to comment out the datasets you don't want to examine in the config file.

The exported files have an extra column on the end which differentiates between the constituent datasets.

Fract DOY in the exported columns runs continuously with 365 (or leap=366) days added on each year.

Some hints on a workable processing strategy:
* You're going to have to use the script "iteratively" to work out the corrections to apply to new data, and which times should be excluded.
* In the config file, comment out datasets you don't want to examine during this process. (i.e. probably everything except the new data)  
* Interrogate the resulting graph to identify periods of time to exclude and the corrections to apply.
* Add these corrections and exclusions to the config file.
* Re-run the script.
* When you're finally happy with the applied corrections you can uncomment datasets in the config file to create a longer date range file, if desired.  



Correcting pole leans
---------------------
This can be attempted by fitting functions to remove the lean and leave the residual.
It's best to write your own script to do this on a case by case basis.
Load in the GEOD file, do alterations on the NEU data in it, and re-save the GEOD file.



Dealing with temporary positions
--------------------------------
E.g. those taken when GPS has powered down so we need to get seasonal/annual displacement from one short survey.

There are likely to be multiple temporary positions in one raw leica file, corresponding to a number of rover locations.

It's probably easiest to do this in a completely separate gps processing folder, also with the gps_config folder pulled down from the SVN repository, in order to avoid filename clashes. Also copy over the relevant base station and orbit files.

First convert the leica file to rinex file(s) using process_rinex. Then window this rinex file into separate rinex files for each rover. Flight timings are very helpful here. You can first check the contents of the rinex file:

  teqc +qc rinex_file_name
  
Check satellite status for each site - if a site is only seeing 4 satellites for a period, or there are significant tracking problems, consider windowing out the poor data.  

To do the windowing:

  teqc -st hhmm00 +dm mm input_rinex_file > output_rinex_file
  
N.b. -st by default assumes everything is in seconds, hence you have to specify the number of seconds to force it to understand hours and minutes. So e.g. to extract Lev6 temporary position from 2012 autumn file, begining at 12pm and continuing for 28 minutes:

  teqc -st 120000 +dm 28 levr_2430_ol_12o > lev6_<doy>0_ol.12o
(Use the same filename format as proper rinex files so that track knows what to look for.)
  
Get the APR coordinates to give to track by uploading the rinex file to:
http://www.geod.nrcan.gc.ca/online_data_e.php
(The other PPP service linked to above doesn't seem to work for these small rinex files. Also, the estimate in the rinex header is also generally incorrect after windowing to each site.)

N.b. PPP services don't necessarily appear to give locations accurate enough by themselves - you do have to process the measurements kinematically through track/process_dgps.
 
Now run process_dgps for each rover site you wish to process.

Copy the GEOD results files into your main GPS processing directory. You'll then be able to concatenate the output GEOD file to the main rover dataset when you run concatenate_geod. 
  
  
  
Converting to velocities
------------------------
This part of the processing strategy remains Matlab-based. Use plot_site_rel_2009_2012_geod.m. Generally filenames, years, rovers etc are hard coded into variables rather than input at the command line.

The bunch of matlab exchange functions which the script requires should all be found in pygps. Add your pygps code folder to the matlab path then run the script from your processing/workspace folder.

Despite the name, the script should work for all years as it has been modified to work out the length of the dataset by itself. However, at the time of writing, the plots it produces have not had their date ticks formatted correctly - the plots still work though.



There are then separate matlab and python scripts for other types of plots.

In particular, plot the newly (re)calculated velocities against those in the last velocities file version, to check that all is as expected.
- Re-calculated velocities sometimes vary slightly to those computed before. On possible explanation, which I haven't tested, is that in plot_site_rel_geod.m a linear fit of all north and east data are calculated for input to the rotation calculation. Presumably the precise definition of this fit will change as more data are collected and processed, causing slight changes to old velocities? AJT, 8 Jan 2014.

	
	
Trimble files
-------------
Net RS files (e.g. those from Aber/Alun's Russell base station) are in T00 format. You'll need to runpkr00 utility to convert them to dat files first. gps.RinexConvert.window_overlap can then process the dat file, e.g.:

>> ipython
> import gps
> rx = gps.RinexConvert()
> rx.window_overlap("file.dat","22:00:00",28)
	



Problems with noisy pseudo-range data (lev7)
--------------------------------------------
Email exchange between AJT and MAK, December 2013-January 2014.
"The other thing that can cause this sort of numerical instability is that the site motion is too tightly constrained. It turns out that is what it looks like was resulting in the NaNs. After fiddling for a couple of days it seems likely there are some other issues as well, and I think the receiver is suspect. I've copied below a .cmd file that seemed to produce sensible results. I think the main problem here is that the pseudorange data are very noisy, and as a result too many cycle slips are flagged and incorrect ambiguities are being fixed. To overcome this I changed the noise for the pseudoranges to 10m and modified some thresholds for detecting slips and fixing them to integers. 

Note: it may well be this works fine for this day but not so well for any other. Please only use it for lev7 during this season. I'd recommend this receiver is serviced before further use. 

A couple of other notes: 

I don't think you're including the details of the different antennas (ANTMOD_FILE, ANTE_OFF). I've used here what is in the rinex headers but I guess these may vary from what actually was installed. This reduces some high frequency noise.
 
using a DCB_FILE is useful for ambiguity fixing. It's in the gamit tables directory. "

See also the lev7-specific .cmd file in gps_config.


	
	

# example Track command file
----------------------------

# Run with -d 356_19 -w 12501
*
* Example track command file for processing San Simeon Data
*

*-----------------------------------------------------------------
* OBS_FILE
* Give the rinex file names to be processed,  The obs_file command
* must be given first.  At least one site must be named as F which
* means the position of the site is fixed.  It recommended to have
* only one fixed site.  The other sites are denoted K for kinematic.
* The site_stats command determines if a site position will be able
* to change during the processing.
* NOTE: At least one blank line must follow the last station in
* the list and there can be no blank lines between the station
* list.

* To use this command file the -d and -w options must used in the
* runstring:
* track -f track.cmd -d 356_19 -w 12501
* The string after the -d and -w replace the <day> and <week> strings
* below

 obs_file 
   trak trak<day>.03o F
   pin1 pin1<day>.03o K
   pomm pomm<day>.03o K
   lows lows<day>.03o K
   crbt crbt<day>.03o K

*-----------------------------------------------------------------
* NAV_FILE
* An orbit file must be given.  In this case we are using the
* mit GPS week 1250 day 1 SP3 orbit file.
* NOTE: when 24-hours of data are processed the sp3 files from
* day before and after need to be concatinated withe sp3 file
* from the day being processed to ensure that that are ephemeris
* entries before and after the last data points.  A simple unix
* cat command can be used to generate the merged files.

 nav_file mit<week>.sp3 sp3


*-----------------------------------------------------------------
* MODE
* A quick method for setting up track is select a mode.  In this case
* since the baselines are up to 300 km long we used mode long.
* Defaults set in this mode can be changes with commands after 
* setting the mode.
 
 mode long

*----------- <END of required inputs> ---------------------------
* The commands below are optional and are used to specific output
* files and change some defaults

* Specific day specific output files

 pos_root TRAK<day>

 res_root TRAK<day>

 sum_file TRAK<day>.sum

* Select the output coordinate type.  For quick assessment and
* looking at relative motions North, East and Up (all in meters)
* are convenient
* Use NEU+GEOD to get both NEU output and geodetic lat, long and height.
* Use GEOD to get just geodetic coordinates.

 out_type NEU

* Running the smoothing filter is reccommended on long baselines
* because this uses all the data for atmospheric delay estimation
* and any non-integer biases (non-resolved biases) are constant
 back smooth

* If the sampling interval is in the rinex files this is not needed
* unless a longer sampling interval is desired
 interval 1

* Small changes to the default parameters to resolve more bias
* parameters for this data.
x float_type 1 1 LC  0.5 1 

* By default and gap in the data introduces a new bias flag in track
* For high rate, telemetered data there are often gaps due to missed
* telemeter.  For this data set, these gaps are less than 2-seconds 
* and here we set gap size before a bias parameter is added to 2-seconds.

 bf_set  2  60

* These lines are not strictly needed.  During this interval PRN 09
* is not visible at TRAK and these data are reported as being removed
* during the track run (bad double difference because there is no 
* combination with TRAK). Adding the explicit edit lines stops these
* messages being output.
 edit_ssv crbt 09 2003 12 22 19 26 55.0 2003 12 22 19 42 30
 edit_ssv lows 09 2003 12 22 19 26 55.0 2003 12 22 19 42 30
 edit_ssv pomm 09 2003 12 22 19 26 55.0 2003 12 22 19 42 30

*--------------------------------------------------------------
* Other commdands that are commonly added
* site_pos -- Sets the apriori coordinates of the sites.  If not used
*             the values from the RINEX header are used.  
*             NOTE: If the rinex header has no coordinates (or zero values)
*             then this command must be used.
* float_type -- Parameters of the sigmas limits and weights to different
*             data types often can be changed for specific data sets to
*             resolve more biases to integers.  Caution should be used 
*             to avoid fixing biases to the wrong integer values.



