#!/usr/bin/env python
"""
SUMMARY
Process TRACK-corrected GNSS data into displacement and velocity time series:
- Filter bad data
- Median filtering to remove outliers
- Gaussian filtering to smooth series
- Differencing to derive velocities
- Export to disk

This script can be used with multiple parquet files; their contents will be used
to produce one single results file.

Run this script in the same directory as your data inputs. 
They are all expected to be in the same directory.

USAGE
On command line:

    python gnss_disp_vel.py -h

To get help on all available options.

In ipython:

    %run /path/to/gnss_disp_vel.py -h
    
As a Notebook:

    In JupyterLab's File Browser pane, right-click this file, then select Open With -> Notebook.
    

DATA INPUTS
Mandatory:
- parquet file(s) (generated by conc_daily_geod.py)
- origin_<site>.csv (generated by calculate_local_origin.py)

Generated if not available:
- rotation_<site>.dat

Optional:
- exclusions_<site>.csv
- pole_corrections_<site>.csv


OUTPUTS 
- h5 file
- csv file with v_24h.

HISTORY
Based on plot_site_rel_2009_2012_geod.m, developed by Matt King, Andrew Sole,
Ian Bartholomew, Andrew Tedstone during 2009-2012.
Created 2022-07-27 AJT.
Option to run as Notebook introduced March 2023, AJT.

"""

import pandas as pd
import os
import numpy as np
import matplotlib.pyplot as plt
import argparse
from scipy.stats import mode
from copy import deepcopy

from gnssice import pp

# ## Input parameters

# +
p = argparse.ArgumentParser('Kinematic GPS: filter positions, calculate \
    trajectories, calculate velocities. Output .h5 files of displacements, \
    CSV file of longer-term velocities, plots of data.')
p.add_argument('site', type=str, help='Name/identifier of site')

p.add_argument('-f', dest='geod_file', type=str, nargs='+', 
    help='Path to GEOD parquet file(s) (output by conc_daily_geod.py)')

p.add_argument('-optspath', type=str, default='', help='Location of options files if \
    not current directory')
p.add_argument('-noexcl', action='store_true')
p.add_argument('-nocorrect', action='store_true')
p.add_argument('-noplot', action='store_true')
p.add_argument('-stake', action='store_true', help='Short-occupation stake mode. \
    Do not apply any filtering or smoothing operations. This is useful for \
    processing short-occupation fixes.')
p.add_argument('-tz', type=str, help='Localise v_24h to timezone.')
p.add_argument('-sample_freq', type=str, default='10s')
p.add_argument('-noconc', action='store_true', help='By default, if multiple \
    parquet files are specified then this script saves a new Parquet file \
    containing them all. Give this flag to prevent the new Parquet file being created.')
p.add_argument('-nbplot', type=str, default='widget', help='If running as a Notebook, \
               defines the matplotlib backend used. Supply widget or inline.')
print('')
# -

# ### Running in a Notebook? Put your parameters in below!

# If running as a Notebook, provide your arguments to the ArgumentParser here.
input_args = ['kanu', '-f', 'kanu_2021_122_2022_135_geod.parquet']
# And also provide the path where all your data are stored. 
# The Notebook will use this to set the working directory.
path_to_data = '/scratch/gnss/kanu'


# ## Identify execution mode

# +
def is_notebook() -> bool:
    """stackoverflow.com/"""
    try:
        shell = get_ipython().__class__.__name__
        if shell == 'ZMQInteractiveShell': 
            return True
        elif shell == 'TerminalInteractiveShell':
            return False
        else:
            return False
    except NameError:
        return False

if is_notebook: 
    print('Notebook mode')
    args = p.parse_args(input_args)
    os.chdir(path_to_data)
    print('Working directory is now %s' %os.getcwd())
    if args.nbplot == 'widget':
        # %matplotlib widget
    elif args.nbplot == 'inline':
        # %matplotlib inline
    else:
        raise ValueError('Unknown matplotlib backend specified for Notebook. Only widget or inline are understood (provided %s)' %args.nbplot)
else:
    print('Terminal mode')
    args = p.parse_args()


# -

# ## Load data and organise output filenames

# Load data and apply timestamp
geod_store = []
for file in args.geod_file:
    geod = pd.read_parquet(file.strip())
    if 'YY' in geod.columns:
        geod.index = pp.create_time_index(geod)
        geod = geod.drop(labels=['YY', 'DOY', 'Seconds'], axis='columns')
        geod_store.append(geod)
        geod = pd.concat(geod_store, axis=0)
    else:
        print('The parquet file provided appears to be a multi-batch file, continuing on this basis...')

# Define output filenames
output_to_pre = '{site}_{ys}_{ds}_{ye}_{de}'.format(
    site=args.site,
    ys=geod.index[0].year,
    ds=geod.index[0].timetuple().tm_yday,
    ye=geod.index[-1].year,
    de=geod.index[-1].timetuple().tm_yday
)

# Save a full concatenated parquet file.
if not args.noconc:
    geod_out = '%s_geod.parquet' %output_to_pre
    # Don't do this save if the proposed output file matches the user-provided file (based on filename)
    if geod_out != args.geod_file[0]:
        if os.path.exists(geod_out):
            os.remove(geod_out)
            print('Old concatenated parquet file %s found, replaced.' %geod_out)
        geod.to_parquet(geod_out)

# Other outputs are appended '_disp' rather than '_geod'.
output_to_pre = output_to_pre + '_disp'
output_to =  '%s.h5' %output_to_pre
output_v24h_csv = '%s_v24h.csv' %output_to_pre

# If there is an existing file, delete it to prevent conflicts.
if os.path.exists(output_to):
    os.remove(output_to)
    print('Old main output file found, deleted.')

# ## Apply user pole corrections

corrections_file = os.path.join(args.optspath, 'pole_corrections_%s.csv' %args.site)
if not args.nocorrect and os.path.exists(corrections_file):
    print('Applying pole corrections')
    geod = pp.apply_pole_corrections()

# ## Apply user exclusions

exclusions_file = os.path.join(args.optspath, 'exclusions_%s.csv' %args.site)
if not args.noexcl and os.path.exists(exclusions_file):
    print('Applying exclusions')
    geod = pp.apply_exclusions(geod, exclusions_file)

# ## Convert coordinates to local north-east-up
# This section calculates coordinates in metres relative to the installation origin of the site.

# Load coordinates of GPS origin point (basically its installation location)
try:
    origin_file = os.path.join(args.optspath, 'origin_%s.csv' %args.site)
    origin = pd.read_csv(origin_file)
except IOError:
    raise IOError('origin_<site>.csv file mandatory but not found.')
origin = origin.iloc[0]

# Calculate North-East-Up
print('Converting to local North-East-Up')
neu = pp.calculate_local_neu(geod, 
    origin['x0'], origin['y0'], origin['z0'], 
    origin['lat0'], origin['lon0'])

# Concatenate results to df
geod_neu = pd.concat((geod, neu), axis=1)

# ## Rotate to along-track (x) and across-track (y) displacement

rfile = os.path.join(args.optspath, 'rotation_%s.dat' %args.site)
if os.path.exists(rfile):
    print('Loading existing rotation matrix')
    r1 = np.loadtxt(rfile)
else:
    print('No rotation matrix file for site exists, creating using entire contents of GEOD file')
    directions = pp.calculate_displacement_trajectory(geod_neu)
    r1 = pp.create_rot_matrix(directions)
    np.savetxt(rfile, r1)

print('Doing rotation')
xy = pp.rotate_to_displacements(geod_neu['East'], geod_neu['North'], r1)
geod_neu_xy = pd.concat((geod_neu, xy), axis=1)

# ## Apply pre-filtering to each period of data
# Here, we primarily remove 'bad' data according to TRACK outputs, and apply median-based-filtering.

if args.stake:
    xyz = geod_neu_xy.filter(items=('x', 'y', 'z'), axis='columns')

if not args.stake:
    ## Identify periods of (i) continuous occupations, (ii) daily occupations, (iii) no occupation
    
    def occ_type(d, threshold_continuous=1000):
        """ Identify and label occupation type for each data period.
        
        This function distinguishes between 'episodic'/'daily' 
        occupration and 'continuous' occupation by means of the number
        of observations made every day. If there are more observations
        than the threshold then the site is considered to be occupied 
        continuously on this day.

        value of 2 = continuous (e.g. 10 sec) occupation
                 1 = 'episodic', e.g. for an hour
                 0 = no occupation

        :param d: data value
        :param threshold_continuous: number of raw observations above which the site
            can be considered to be recording continuously.
        :returns: 0, 1 or 2
        """
        if d > threshold_continuous: return 2
        if d < threshold_continuous and d > 0: return 1
        return 0

    def process_daily_occups(df):
        """ The filtering processes for episodic/daily observations. """
        # Find the modal hour of this period's observations
        df = pp.filter_positions(df, thresh_N=3, thresh_NotF=3)
        #occ_time = '%sH' %int(mode(df.index.hour)[0])
        # Use this modal hour to place the index of the mean positions at day:hour.
        #df = df.resample('1D', offset=occ_time).mean().rolling('10D', center=True).mean()
        return df

    def process_cont_occups(df):
        """ The filtering processes for continuous observations. """
        # Do initial filtering
        print('Filtering out bad positions (according to RMS etc)')
        df = pp.filter_positions(df)
        df = pp.remove_displacement_outliers(df, args.sample_freq, iterations=2)
        return df

    print('*** Checking for different occupation periods ***')
    # Count number of daily observations
    counts = geod_neu_xy.x.resample('1D').count().to_frame()
    counts['type'] = counts['x'].apply(occ_type)
    counts['type_roll'] = counts['type'].rolling('3D', center=True).max() #apply(lambda x: mode(x)[0])

    # Find change dates
    detect = np.abs(counts['type_roll'].shift(1) - counts['type_roll'])
    dates = detect[detect > 0].index.to_list()

    # Get associated period types
    period_types = counts['type_roll'][detect > 0].to_list()

    # Create equivalent 'starting' lists that include the first period's type
    period_types_start = deepcopy(period_types)
    period_types_start.insert(0, counts['type_roll'].iloc[0])
    dates_start = deepcopy(dates)
    dates_start.insert(0, geod_neu_xy.index[0])

    # Add dummy values to make lists same size as _start variants
    period_types.append(0)
    dates.append(0)

    # Process each identified period of data in turn.
    store = []
    for d_st, d_en, s in zip(dates_start, dates, period_types_start):
        
        # Get the data from this period...
        if d_en == 0:
            pxyz = geod_neu_xy[d_st:]
            d_en = geod_neu_xy.index[-1]
        else:
            # pandas indexing is inclusive, so to prevent using the same data twice we
            # need to force the indexing to be 'exclusive'.
            d_en = d_en - pd.Timedelta(days=1)

            if s == 2:
                # Case of continuous data
                pxyz = geod_neu_xy[d_st:d_en]
            elif s == 1:
                # Try to add a couple of days from the continuous time series, to improve rolling window smoothing.
                # This may need re-thinking!
                pxyz = geod_neu_xy[d_st:d_en+pd.Timedelta(days=2)]
            elif s == 0:
                pass
            else:
                raise ValueError('Unknown occupation type.')

        message = 'Period {ds} to {de}: treating as {{type}} occupations.'.format(
                ds=d_st.strftime('%Y-%m-%d'),
                de=d_en.strftime('%Y-%m-%d'))

        # Now apply the correct type of filtering for the period type.
        if s == 1:
            # do daily processing
            print(message.format(type='daily'))
            pxyz = process_daily_occups(pxyz)
        elif s == 2:
            # do continuous processing
            print(message.format(type='continuous'))
            pxyz = process_cont_occups(pxyz)
            occstr = 'continuous'
        elif s == 0:
            print(message.format(type='empty/ignore'))
            pass
        else:
            raise ValueError('Unknown occupation type.')

        # Make sure that we only save back data for the period,
        # without any data which may have been prepended/appended
        # to help with filtering edge effects.
        pxyz = pxyz[d_st:d_en]

        store.append(pxyz)

    filtd = pd.concat(store, axis=0)
    print('*** End of period-based processing ***')

# ## Smoothing the whole time series

if not args.stake:
    # Restore to original frequency and interpolate; adds a flag column named 'interpolated'
    print('Regularising whole series')
    filtd_i = pp.regularise(filtd, args.sample_freq)

    # Do Gaussian filtering - this df does not have interpolated column
    print('Gaussian filtering whole series')
    filtd_disp = pp.smooth_displacement(filtd_i, 7200) #7200secs ~ 2hours

    # Sub-set data to retain only original data samples (modified by Gaussian filtering)
    xyz = filtd_disp.filter(items=('x', 'y', 'z'), axis='columns')
    xyz = xyz[filtd_i.interpolated == 0]

# ## Calculating and saving velocities

if not args.stake:
    # Calculate velocities
    print('Calculating velocities')
    v_24h = pp.calculate_daily_velocities(filtd_disp['x'], tz=args.tz)
    maxperday = pd.Timedelta('1D') / pd.Timedelta(args.sample_freq)
    dayperc = 100 / maxperday * filtd_i.interpolated[filtd_i.interpolated == 0].resample('1D').count()
    v_24h = pd.DataFrame({'v_24h':v_24h, 'obs_cover_percent':dayperc}, index=v_24h.index)

    # Longer-term velocities, following Doyle 2014 approach
    d6h = filtd_disp.resample('6H').mean()
    p15d = d6h.resample('15D').first()
    v15d = (p15d.x.shift(1) - p15d.x) * pp.v_mult('15D')

    # Regression-based approach
    # !!TODO!! implement a mix of Doyle for continuous and this for daily occs...
    # Want to apply this on the most raw data possible? i.e. not regliarsed and smoothed, as these
    # are not helpful operations on daily data
    # This doesn't really work on data filtered with a 'standard' approach - 
    # only currently works with geod_neu_xy. What would a good filtering strategy for
    # daily data look like?
    # Or do these need to be reprocessed by TRACK with ambiguities fixed?
    p30d = filtd.x.resample('30D').apply(pp.position_by_regression)
    v30d = (p30d.shift(1) - p30d) * pp.v_mult('30D')

    # consider calculating uncertainties? 
    # To what degree is this RMS? How do uncertainties reduce by avging?

    v_6h = pp.calculate_short_velocities(filtd_disp['x'], '6H')

    v_24h.to_hdf(output_to, 'v_24h', format='table')
    v_24h.to_csv(output_v24h_csv)
    v_6h.to_hdf(output_to, 'v_6h', format='table')

    print('24-hour velocities also exported to: %s.' %output_v24h_csv)

# ## Save displacements

# Save displacements to disk
xyz.to_hdf(output_to, 'xyz', format='table')
print('Finished.')
print('Main output file: %s.' %output_to)

# ## Development feature: subset the parquet file to a smaller file
# Use this to save a smaller dataset which you can use with this Notebook during testing of filtering procedures. This saves a dataset which contains only unmodified data, i.e. no filtering has been applied to it!

if is_notebook():
    # Put in your dates of interest here
    smaller_data = geod.loc['2021-10-01':'2021-12-31']
    # Then let's work out the correct filename
    smaller_fn = '{site}_{ys}_{ds}_{ye}_{de}.parquet'.format(
        site=args.site,
        ys=smaller_data.index[0].year,
        ds=smaller_data.index[0].timetuple().tm_yday,
        ye=smaller_data.index[-1].year,
        de=smaller_data.index[-1].timetuple().tm_yday
    )
    smaller_data.to_parquet(smaller_fn)
    print('Output to %s.' %smaller_fn)

# ## Under development: calculating velocities over flexible window lengths

### Try a flexible window length approach on raw data.
filt_manual = pp.filter_positions(geod_neu_xy)
# Doyle et al use 6H positions
# Can I use regression across several days to estimate the accuracy/quality of the single-day-obs used?
fmanres = filt_manual.x[(filt_manual.index.hour >= 9) & (filt_manual.index.hour <= 15)].resample('1D').mean()

#fmanres['2021-12-04'] = np.nan
#fmanres['2021-12-05'] = np.nan
## Velocity over a minimum window length, looking ahead for the first position value at least x days away from current obs, otherwise more.
# need to start with a 'good' date, how best to identify this?
procdate = fmanres.index[0] #pd.Timestamp('2021-05-10')
store = []
while True:
    loc_today = fmanres.loc[procdate]
    loc_next = fmanres.loc[procdate + pd.Timedelta(days=15):]
    loc_next = loc_next.dropna()
    if len(loc_next) == 0:
        print('End of series')
        break
    date_next = loc_next.index[0]
    loc_next = loc_next.iloc[0]
    print(loc_next)
    tdiff = date_next - procdate
    diff = np.abs(loc_next - loc_today)
    vel = diff * pp.v_mult('%sD'%tdiff.days)
    store.append(dict(start=procdate, finish=date_next, disp=diff, velocity=vel))
    procdate = date_next
outp = pd.DataFrame(store)


# ## Diagnostic plots of final outputs

if not args.noplot:
    try:
        import seaborn as sns
        sns.set_style('whitegrid')
    except:
        pass

# ### For stakes

if not args.noplot and args.stake:
    plt.figure()
    plt.plot(geod_neu_xy.x, geod_neu_xy.y, '.', color='gray', alpha=0.3, label='GEOD (after exclusions)')
    plt.title('Stake/Quick-Pos X-Y')
    plt.savefig('%s_xy.png' %output_to_pre, dpi=300)

# ### For GPS stations

if not args.noplot and not args.stake:
    do_plot = True

# Plot X versus Y
if do_plot:
    plt.figure()
    plt.plot(geod_neu_xy.x, geod_neu_xy.y, '.', color='gray', alpha=0.3, label='GEOD (after exclusions)')
    plt.plot(filtd.x, filtd.y, '.', color='tab:blue', alpha=0.3, label='Filtered, Smoothed')
    plt.plot(xyz.x, xyz.y, '.', color='tab:purple', alpha=0.3, label='Final retained epochs')
    plt.xlabel('Metres')
    plt.ylabel('Metres')
    plt.title('%s X - Y' %args.site)
    plt.legend()
    plt.savefig('%s_xy.png' %output_to_pre, dpi=300)

# Plot Time versus X
if do_plot:
    plt.figure()
    plt.plot(geod_neu_xy.index, geod_neu_xy.x, '.', color='gray', alpha=0.3, label='GEOD (after exclusions)')
    plt.plot(filtd_disp.index, filtd_disp.x, '.', color='tab:blue', alpha=0.1, label='Filtered, Smoothed')
    plt.plot(xyz.index, xyz.x, '.', color='tab:purple', alpha=0.3, label='Final retained epochs')
    plt.ylabel('Metres')
    plt.title('%s X - Time' %args.site)
    plt.savefig('%s_xt.png' %output_to_pre, dpi=300)

# Plot Y versus Time
if do_plot:
    plt.figure()
    plt.plot(geod_neu_xy.y, geod_neu_xy.index, '.', color='gray', alpha=0.3, label='GEOD (after exclusions)')
    plt.plot(filtd_disp.y, filtd_disp.index, '.', color='tab:blue', alpha=0.1, label='Filtered, Smoothed')
    plt.plot(xyz.y, xyz.index, '.', color='tab:purple', alpha=0.3, label='Final retained epochs')
    plt.xlabel('Metres')
    plt.title('%s Time - Y' %args.site) 
    plt.savefig('%s_ty.png' %output_to_pre, dpi=300)

# Plot Time versus Z
if do_plot:
    plt.figure()
    plt.plot(geod_neu_xy.index, geod_neu_xy.z, '.', color='gray', alpha=0.3, label='GEOD (after exclusions)')
    plt.plot(filtd_disp.index, filtd_disp.z, '.', color='tab:blue', alpha=0.1, label='Filtered, Smoothed')
    plt.plot(xyz.index, xyz.z, '.', color='tab:purple', alpha=0.3, label='Final retained epochs')
    plt.ylabel('Metres')
    plt.title('%s Time - Z' %args.site) 
    plt.savefig('%s_tz.png' %output_to_pre, dpi=300)

# Plot 24-hour differenced velocities
if do_plot:
    plt.figure()
    ax1 = plt.subplot(211)
    v_24h.v_24h.plot(ax=ax1, alpha=0.5)
    # Use steps-pre: velocity calculation for day0 is (X_day1 - Xday0), so the step will show what happens that day.
    v_24h.v_24h.plot(drawstyle='steps-pre', ax=ax1)
    plt.title('%s 24H velocity' %args.site)
    plt.ylabel('m/yr')
    ax2 = plt.subplot(212, sharex=ax1)
    v_24h.obs_cover_percent.plot(drawstyle='steps-pre', ax=ax2)
    plt.ylabel(r'% daily obs cover')
    plt.savefig('%s_v24h.png' %output_to_pre, dpi=300)

# Plot instantaneous 6-hour differenced velocities
if do_plot:       
    plt.figure()
    v_6h.plot()
    plt.title('%s Instantaneous 6H velocity' %args.site)
    plt.ylabel('m/yr')
    plt.savefig('%s_v6h.png' %output_to_pre, dpi=300)

# Plot velocities over other window lengths
if do_plot:       
    plt.figure()
    v15d.plot(drawstyle='steps-pre', label='15d')
    v30d.plot(drawstyle='steps-pre', label='30d')
    plt.title('%s 15/30 Day velocity' %args.site)
    plt.ylabel('m/yr')
    plt.legend()
    plt.savefig('%s_v15d.png' %output_to_pre, dpi=300)
