#!/usr/bin/env python
"""
SUMMARY
Process TRACK-corrected GNSS data into displacement and velocity time series:
- Filter bad data
- Median filtering to remove outliers
- Gaussian filtering to smooth series
- Differencing to derive velocities
- Export to disk

This script can be used with multiple parquet files; their contents will be used
to produce one single results file.

DATA INPUTS
Mandatory:
- parquet file(s) (generated by conc_daily_geod.py)
- origin_<site>.csv (generated by calculate_local_origin.py)

Generated if not available:
- rotation_<site>.dat

Optional:
- exclusions_<site>.csv
- pole_corrections_<site>.csv


OUTPUTS 
- h5 file
- csv file with v_24h.

HISTORY
Based on plot_site_rel_2009_2012_geod.m, developed by Matt King, Andrew Sole,
Ian Bartholomew, Andrew Tedstone during 2009-2012.
Created 2022-07-27 AJT.

"""

import pandas as pd
import os
import numpy as np
import matplotlib.pyplot as plt
import argparse
from scipy.stats import mode
from copy import deepcopy

from gnssice import pp

p = argparse.ArgumentParser('Kinematic GPS: filter positions, calculate trajectories, calculate velocities.')
p.add_argument('site', type=str, help='Name/identifier of site')

p.add_argument('-f', dest='geod_file', type=str, nargs='+', 
    help='Path to GEOD parquet file(s) (output by conc_daily_geod.py)')

p.add_argument('-optspath', type=str, default='', help='Location of options files if not current directory')
p.add_argument('-noexcl', action='store_true')
p.add_argument('-nocorrect', action='store_true')
p.add_argument('-noplot', action='store_true')
p.add_argument('-stake', action='store_true', help='Short-occupation stake mode. Do not apply any filtering or smoothing operations. This is useful for processing short-occupation fixes.')
p.add_argument('-tz', type=str, help='Localise v_24h to timezone.')
p.add_argument('-sample_freq', type=str, default='10s')

args = p.parse_args()

# Load data and apply timestamp
geod_store = []
for file in args.geod_file:
    geod = pd.read_parquet(file)
    geod.index = pp.create_time_index(geod)
    geod = geod.drop(labels=['YY', 'DOY', 'Seconds'], axis='columns')
    geod_store.append(geod)
geod = pd.concat(geod_store, axis=0)

# Define output filenames
output_to_pre = '{site}_{ys}_{ds}_{ye}_{de}_geod'.format(
    site=args.site,
    ys=geod.index[0].year,
    ds=geod.index[0].timetuple().tm_yday,
    ye=geod.index[-1].year,
    de=geod.index[-1].timetuple().tm_yday
)
output_to =  '%s.h5' %output_to_pre
output_v24h_csv = '%s_v24h.csv' %output_to_pre

# If there is an existing file, delete it to prevent conflicts.
if os.path.exists(output_to):
    os.remove(output_to)
    print('Old main output file found, deleted.')

# Apply user pole corrections
corrections_file = os.path.join(args.optspath, 'pole_corrections_%s.csv' %args.site)
if not args.nocorrect and os.path.exists(corrections_file):
    print('Applying pole corrections')
    geod = pp.apply_pole_corrections()

# Apply user exclusions
exclusions_file = os.path.join(args.optspath, 'exclusions_%s.csv' %args.site)
if not args.noexcl and os.path.exists(exclusions_file):
    print('Applying exclusions')
    geod = pp.apply_exclusions(geod, exclusions_file)

# Load coordinates of GPS origin point (basically its installation location)
try:
    origin_file = os.path.join(args.optspath, 'origin_%s.csv' %args.site)
    origin = pd.read_csv(origin_file)
except IOError:
    raise IOError('origin_<site>.csv file mandatory but not found.')
origin = origin.iloc[0]

# Calculate North-East-Up
print('Converting to local North-East-Up')
neu = pp.calculate_local_neu(geod, 
    origin['x0'], origin['y0'], origin['z0'], 
    origin['lat0'], origin['lon0'])

# Concatenate results to df
geod_neu = pd.concat((geod, neu), axis=1)

# Rotate to along-track (x) and across-track (y) displacement
rfile = os.path.join(args.optspath, 'rotation_%s.dat' %args.site)
if os.path.exists(rfile):
    print('Loading existing rotation matrix')
    r1 = np.loadtxt(rfile)
else:
    print('No rotation matrix file for site exists, creating using entire contents of GEOD file')
    directions = pp.calculate_displacement_trajectory(geod_neu)
    r1 = pp.create_rot_matrix(directions)
    np.savetxt(rfile, r1)

print('Doing rotation')
xy = pp.rotate_to_displacements(geod_neu['East'], geod_neu['North'], r1)
geod_neu_xy = pd.concat((geod_neu, xy), axis=1)

if args.stake:
    xyz = geod_neu_xy.filter(items=('x', 'y', 'z'), axis='columns')
else:
    ## Identify periods of (i) continuous occupations, (ii) daily occupations, (iii) no occupation
    
    def occ_type(d, threshold_continuous=1000):
        """ Define daily occupation type.

        value of 2 = continuous (e.g. 10 sec) occupation
                 1 = 'episodic', e.g. for an hour
                 0 = no occupation

        :param d: data value
        :param threshold_continuous: number of raw observations above which the site
            can be considered to be recording continuously.
        :returns: 0, 1 or 2
        """
        if d > threshold_continuous: return 2
        if d < threshold_continuous and d > 0: return 1
        return 0

    def process_daily_occups(df):
        # Find the modal hour of this period's observations
        occ_time = '%sH' %int(mode(df.index.hour)[0])
        # Use this modal hour to place the index of the mean positions at day:hour.
        df = df.resample('1D', offset=occ_time).mean().rolling('5D', center=True).mean()
        return df

    def process_cont_occups(df):
        # Do initial filtering
        print('Filtering out bad positions (according to RMS etc)')
        df = pp.filter_positions(df)
        df = pp.remove_displacement_outliers(df, args.sample_freq, iterations=2)
        return df

    print('*** Checking for different occupation periods ***')
    # Count number of daily observations
    counts = geod_neu_xy.x.resample('1D').count().to_frame()
    counts['type'] = counts['x'].apply(occ_type)
    counts['type_roll'] = counts['type'].rolling('3D', center=True).max() #apply(lambda x: mode(x)[0])

    # Find change dates
    detect = np.abs(counts['type_roll'].shift(1) - counts['type_roll'])
    dates = detect[detect > 0].index.to_list()

    # Get associated segment types
    seg_types = counts['type_roll'][detect > 0].to_list()

    # Create equivalent 'starting' lists that include the first date/segment type
    seg_types_start = deepcopy(seg_types)
    seg_types_start.insert(0, counts['type_roll'].iloc[0])
    dates_start = deepcopy(dates)
    dates_start.insert(0, geod_neu_xy.index[0])

    # Add dummy values to make lists same size as _start variants
    seg_types.append(0)
    dates.append(0)

    store = []
    for d_st, d_en, s in zip(dates_start, dates, seg_types_start):
        
        if d_en == 0:
            pxyz = geod_neu_xy[d_st:]
            d_en = geod_neu_xy.index[-1]
        else:
            # pandas indexing is inclusive, so to prevent using the same data twice we
            # need to force the indexing to be 'exclusive'.
            d_en = d_en - pd.Timedelta(days=1)

            if s == 2:
                pxyz = geod_neu_xy[d_st:d_en]
            elif s == 1:
                # Try to add a couple of days from the continuous time series, to improve rolling window smoothing.
                pxyz = geod_neu_xy[d_st:d_en+pd.Timedelta(days=2)]
            elif s == 0:
                pass
            else:
                raise ValueError('Unknown occupation type.')

        message = 'Period {ds} to {de}: treating as {{type}} occupations.'.format(
                ds=d_st.strftime('%Y-%m-%d'),
                de=d_en.strftime('%Y-%m-%d'))

        if s == 1:
            # do daily processing
            print(message.format(type='daily'))
            pxyz = process_daily_occups(pxyz)
        elif s == 2:
            # do continuous processing
            print(message.format(type='continuous'))
            pxyz = process_cont_occups(pxyz)
            occstr = 'continuous'
        elif s == 0:
            print(message.format(type='empty/ignore'))
            pass
        else:
            raise ValueError('Unknown occupation type.')

        pxyz = pxyz[d_st:d_en]

        store.append(pxyz)

    filtd = pd.concat(store, axis=0)
    print('*** End of period-based processing ***')

    # Restore to original frequency and interpolate; adds a flag column named 'interpolated'
    print('Regularising whole series')
    filtd_i = pp.regularise(filtd, args.sample_freq)

    # Do Gaussian filtering - this df does not have interpolated column
    print('Gaussian filtering whole series')
    filtd_disp = pp.smooth_displacement(filtd_i, 7200) #7200secs ~ 2hours

    # Sub-set data to retain only original data samples (modified by Gaussian filtering)
    xyz = filtd_disp.filter(items=('x', 'y', 'z'), axis='columns')
    xyz = xyz[filtd_i.interpolated == 0]

    # Calculate velocities
    print('Calculating velocities')
    v_24h = pp.calculate_daily_velocities(filtd_disp['x'], tz=args.tz)
    maxperday = pd.Timedelta('1D') / pd.Timedelta(args.sample_freq)
    dayperc = 100 / maxperday * filtd_i.interpolated[filtd_i.interpolated == 0].resample('1D').count()
    v_24h = pd.DataFrame({'v_24h':v_24h, 'obs_cover_percent':dayperc}, index=v_24h.index)


    # Longer-term velocities, following Doyle 2014 approach
    d6h = filtd_disp.resample('6H').mean()
    p15d = d6h.resample('15D').first()
    v15d = (p15d.x.shift(1) - p15d.x) * pp.v_mult('15D')

    # consider calculating uncertainties? 
    # To what degree is this RMS? How do uncertainties reduce by avging?

    v_6h = pp.calculate_short_velocities(filtd_disp['x'], '6H')

    v_24h.to_hdf(output_to, 'v_24h', format='table')
    v_24h.to_csv(output_v24h_csv)
    v_6h.to_hdf(output_to, 'v_6h', format='table')

    print('24-hour velocities also exported to: %s.' %output_v24h_csv)

# Save displacements to disk
xyz.to_hdf(output_to, 'xyz', format='table')

print('Finished.')
print('Main output file: %s.' %output_to)


##################################################################

if not args.noplot:

    try:
        import seaborn as sns
        sns.set_style('whitegrid')
    except:
        pass

    if args.stake:
        plt.figure()
        plt.plot(geod_neu_xy.x, geod_neu_xy.y, '.', color='gray', alpha=0.3, label='GEOD (after exclusions)')
        plt.title('Stake/Quick-Pos X-Y')
        plt.savefig('%s_xy.png' %output_to_pre, dpi=300)

    else:
        plt.figure()
        plt.plot(geod_neu_xy.x, geod_neu_xy.y, '.', color='gray', alpha=0.3, label='GEOD (after exclusions)')
        plt.plot(filtd.x, filtd.y, '.', color='tab:blue', alpha=0.3, label='Filtered, Smoothed')
        plt.plot(xyz.x, xyz.y, '.', color='tab:purple', alpha=0.3, label='Final retained epochs')
        plt.xlabel('Metres')
        plt.ylabel('Metres')
        plt.title('%s X - Y' %args.site)
        plt.legend()
        plt.savefig('%s_xy.png' %output_to_pre, dpi=300)

        plt.figure()
        plt.plot(geod_neu_xy.index, geod_neu_xy.x, '.', color='gray', alpha=0.3, label='GEOD (after exclusions)')
        plt.plot(filtd_disp.index, filtd_disp.x, '.', color='tab:blue', alpha=0.1, label='Filtered, Smoothed')
        plt.plot(xyz.index, xyz.x, '.', color='tab:purple', alpha=0.3, label='Final retained epochs')
        plt.ylabel('Metres')
        plt.title('%s X - Time' %args.site)
        plt.savefig('%s_xt.png' %output_to_pre, dpi=300)

        plt.figure()
        plt.plot(geod_neu_xy.y, geod_neu_xy.index, '.', color='gray', alpha=0.3, label='GEOD (after exclusions)')
        plt.plot(filtd_disp.y, filtd_disp.index, '.', color='tab:blue', alpha=0.1, label='Filtered, Smoothed')
        plt.plot(xyz.y, xyz.index, '.', color='tab:purple', alpha=0.3, label='Final retained epochs')
        plt.xlabel('Metres')
        plt.title('%s Time - Y' %args.site) 
        plt.savefig('%s_ty.png' %output_to_pre, dpi=300)

        plt.figure()
        plt.plot(geod_neu_xy.index, geod_neu_xy.z, '.', color='gray', alpha=0.3, label='GEOD (after exclusions)')
        plt.plot(filtd_disp.index, filtd_disp.z, '.', color='tab:blue', alpha=0.1, label='Filtered, Smoothed')
        plt.plot(xyz.index, xyz.z, '.', color='tab:purple', alpha=0.3, label='Final retained epochs')
        plt.ylabel('Metres')
        plt.title('%s Time - Z' %args.site) 
        plt.savefig('%s_tz.png' %output_to_pre, dpi=300)

        plt.figure()
        ax1 = plt.subplot(211)
        v_24h.v_24h.plot(ax=ax1, alpha=0.5)
        # Use steps-pre: velocity calculation for day0 is (X_day1 - Xday0), so the step will show what happens that day.
        v_24h.v_24h.plot(drawstyle='steps-pre', ax=ax1)
        plt.title('%s 24H velocity' %args.site)
        plt.ylabel('m/yr')
        ax2 = plt.subplot(212, sharex=ax1)
        v_24h.obs_cover_percent.plot(drawstyle='steps-pre', ax=ax2)
        plt.ylabel(r'% daily obs cover')
        plt.savefig('%s_v24h.png' %output_to_pre, dpi=300)

        # plt.figure()
        # v_6h.plot()
        # plt.title('%s Instantaneous 6H velocity' %args.site)
        # plt.ylabel('m/yr')
        # plt.savefig('%s_v6h.png' %output_to_pre, dpi=300)

        plt.figure()
        v15d.plot(drawstyle='steps-pre')
        plt.title('%s 15 Day velocity' %args.site)
        plt.ylabel('m/yr')
        plt.savefig('%s_v15d.png' %output_to_pre, dpi=300)

        plt.show()
