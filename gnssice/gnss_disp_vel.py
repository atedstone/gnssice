# # gnss_disp_vel.py
#
# Process TRACK-corrected GNSS data into displacement and velocity time series:
# - Filter bad data
# - Median filtering to remove outliers
# - Gaussian filtering to smooth series
# - Differencing to derive velocities
# - Export to disk
#
# This script can be used with multiple parquet files; their contents will be used
# to produce one single results file.
#
# This script looks for Level-1 inputs in `$GNSS_L1DIR/site` and stores outputs to
# `$GNSS_L2DIR/site`. By default it will look for the origin, rotation and exclusion 
# files in `$GNSS_L2DIR/site.`
#
# ## USAGE
# On command line:
#
#     python gnss_disp_vel.py -h
#
# To get help on all available options.
#
# In ipython:
#
#     # # %run /path/to/gnss_disp_vel.py -h
#     
# As a Notebook:
#
#     In JupyterLab's File Browser pane, right-click this file, then select Open With -> Notebook.
#     
#
# ## DATA INPUTS
# Mandatory:
# - parquet file (generated by `export_level1.py`)
# - `origin_<site>.csv` (generated by `calculate_local_origin.py`)
#
# Generated if not available:
# - `rotation_<site>.dat`
#
# Optional:
# - `exclusions_<site>.csv`
# - `pole_corrections_<site>.csv`
#
#
# ## OUTPUTS 
# - h5 file
# - csv file with v_24h.
#
# ## HISTORY
# Based on `plot_site_rel_2009_2012_geod.m`, developed by Matt King, Andrew Sole,
# Ian Bartholomew, Andrew Tedstone during 2009-2012.
# Created 2022-07-27 AJT.
# Option to run as Notebook introduced March 2023, AJT.
# Updates and improvements including to pp.py July/August 2025, AJT.
#
#

import pandas as pd
import os
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import argparse
from scipy.stats import mode
from copy import deepcopy
from glob import glob

from gnssice import pp

# ## Input parameters

# +
p = argparse.ArgumentParser('Kinematic GPS: filter positions, calculate \
    trajectories, calculate velocities. Output .h5 files of displacements, \
    CSV file of longer-term velocities, plots of data.')
p.add_argument('site', type=str, help='Name/identifier of site')
p.add_argument('-optspath', type=str, default=None, help='Location of options files (origin, exclusions...) if \
    not $GNSS_L2DIR')
p.add_argument('-noexcl', action='store_true')
p.add_argument('-nocorrect', action='store_true')
p.add_argument('-noplot', action='store_true')
p.add_argument('-stake', action='store_true', help='Short-occupation stake mode. \
    Do not apply any filtering or smoothing operations. This is useful for \
    processing short-occupation fixes.')
p.add_argument('-tz', type=str, help='Localise v_24h to timezone.')
p.add_argument('-sample_freq', type=str, default='10s')
p.add_argument('-v6h', action='store_true', help='If set then also compute sliding 6h velocities for the site.')

p.add_argument('-nbplot', type=str, default='widget', help='If running as a Notebook, \
               defines the matplotlib backend used. Supply widget or inline.')
print('')

# -

# ### Running in a Notebook? Put your parameters in below!

# If running as a Notebook, provide your arguments to the ArgumentParser here.
#input_args = ['f004', '-stake']
#input_args = ['lev5', '-v6h']
input_args = ['lev6']


# ## Identify execution mode

# +
def is_notebook() -> bool:
    """stackoverflow.com/"""
    try:
        shell = get_ipython().__class__.__name__
        print('shell:', shell)
        if shell == 'ZMQInteractiveShell': 
            return True
        elif shell == 'TerminalInteractiveShell':
            return False
        else:
            return False
    except NameError:
        return False

if is_notebook(): 
    print('Notebook mode')
    args = p.parse_args(input_args)
    if args.nbplot == 'widget':
        print('widget')
        # %matplotlib widget
    elif args.nbplot == 'inline':
        print('inline')
        # %matplotlib inline
    else:
        raise ValueError('Unknown matplotlib backend specified for Notebook. Only widget or inline are understood (provided %s)' %args.nbplot)
    # %load_ext autoreload
    # %autoreload 2
    # %aimport gnssice
else:
    print('Terminal mode')
    args = p.parse_args()


# +
#WORKING_DIRECTORY = os.path.join(os.environ['GNSS_WORK'], args.site)
#os.chdir(WORKING_DIRECTORY)
#print('Working directory is now %s' %os.getcwd())

path_output_L2 = os.path.join(os.environ['GNSS_L2DIR'], args.site)
#Path(path_output_L2).mkdir(exist_ok=True)
print(f'Level-2 Directory: {path_output_L2}')

# By default, find the options files (e.g. exclusions) in the Level-2 directory.
if args.optspath is None:
    args.optspath = path_output_L2
# -

# ## Load Level-1 data

level1_path = os.path.join(os.environ['GNSS_L1DIR'], args.site, '*_geod.parquet')
f = glob(level1_path)
if len(f) == 0:
    raise FileNotFoundError(f'No level-1 file found for {args.site}.')
elif len(f) > 1:
    raise ValueError(f'More than one level-1 file found for {args.site}.')
else:
    f = f[0]
print(f'Loading Level-1 file: {f}...')
#f = '/scratch/hislide-processing/lev6_2023_127_2023_273_geod.parquet'
#f = '/scratch/hislide-level1/lev5/lev5_2021_129_2023_128_geod.parquet'
geod = pd.read_parquet(f.strip())
geod = pp.update_legacy_geod_col_names(geod)

# ## Apply user pole corrections

corrections_file = os.path.join(args.optspath, 'pole_corrections_%s.csv' %args.site)
if not args.nocorrect and os.path.exists(corrections_file):
    print('Applying pole corrections')
    geod = pp.apply_pole_corrections()

# ## Convert coordinates to local north-east-up
# This section calculates coordinates in metres relative to the installation origin of the site.

# Load coordinates of GPS origin point (basically its installation location)
try:
    origin_file = os.path.join(args.optspath, 'origin_%s.csv' %args.site)
    origin = pd.read_csv(origin_file)
except IOError:
    raise IOError('origin_<site>.csv file mandatory but not found.')
origin = origin.iloc[0]

# Calculate North-East-Up
print('Converting to local North-East-Up')
neu = pp.calculate_local_neu(geod, 
    origin['x0'], origin['y0'], origin['z0'], 
    origin['lat0'], origin['lon0'])

# Concatenate results to df
geod_neu = pd.concat((geod, neu), axis=1)

# ## Rotate to along-track (x) and across-track (y) displacement

rfile = os.path.join(args.optspath, 'rotation_%s.dat' %args.site)
if os.path.exists(rfile):
    print('Loading existing rotation matrix')
    r1 = np.loadtxt(rfile)
else:
    print('No rotation matrix file for site exists, creating using entire contents of GEOD file')
    directions = pp.calculate_displacement_trajectory(geod_neu)
    r1 = pp.create_rot_matrix(directions)
    np.savetxt(rfile, r1)

print('Doing rotation')
xy = pp.rotate_to_displacements(geod_neu['East_m'], geod_neu['North_m'], r1)
geod_neu_xy = pd.concat((geod_neu, xy), axis=1)

# ## Apply user exclusions

exclusions_file = os.path.join(args.optspath, 'exclusions_%s.csv' %args.site)
if not args.noexcl and os.path.exists(exclusions_file):
    print('Applying exclusions')
    geod_neu_xy = pp.apply_exclusions(geod_neu_xy, exclusions_file)
else:
    geod_neu_xy['exclude'] = False

# ## Apply pre-filtering to each period of data
# Here, we primarily remove 'bad' data according to TRACK outputs, and apply median-based-filtering.

if args.stake:
    xyz = geod_neu_xy[geod_neu_xy.exclude == False].filter(items=('x_m', 'y_m', 'z_m'), axis='columns')

if not args.stake:
    ## Identify periods of (i) continuous occupations, (ii) daily occupations, (iii) no occupation
    
    def occ_type(d, threshold_continuous=1000):
        """ Identify and label occupation type for each data period.
        
        This function distinguishes between 'episodic'/'daily' 
        occupration and 'continuous' occupation by means of the number
        of observations made every day. If there are more observations
        than the threshold then the site is considered to be occupied 
        continuously on this day.

        value of 2 = continuous (e.g. 10 sec) occupation
                 1 = 'episodic', e.g. for an hour
                 0 = no occupation

        :param d: data value
        :param threshold_continuous: number of raw observations above which the site
            can be considered to be recording continuously.
        :returns: 0, 1 or 2
        """
        if d > threshold_continuous: return 2
        if d < threshold_continuous and d > 0: return 1
        return 0

    def process_daily_occups(df):
        """ The filtering processes for episodic/daily observations. """
        # Find the modal hour of this period's observations
        df = pp.filter_positions(df, thresh_N=3, thresh_NotF=3)
        #occ_time = '%sH' %int(mode(df.index.hour)[0])
        # Use this modal hour to place the index of the mean positions at day:hour.
        #df = df.resample('1D', offset=occ_time).mean().rolling('10D', center=True).mean()
        return df

    def process_cont_occups(df):
        """ The filtering processes for continuous observations. """
        # Do initial filtering
        print('Filtering out bad positions (according to RMS etc)')
        df = pp.filter_positions(df)
        df = pp.remove_displacement_outliers(df, args.sample_freq, iterations=2, median_win='6h')
        return df

    print('*** Checking for different occupation periods ***')
    # Count number of daily observations
    counts = geod_neu_xy[geod_neu_xy.exclude == False].x_m.resample('1D').count().to_frame()
    counts['type'] = counts['x_m'].apply(occ_type)
    counts['type_roll'] = counts['type'].rolling('3D', center=True).max() #apply(lambda x: mode(x)[0])

    # Find change dates
    detect = np.abs(counts['type_roll'].shift(1) - counts['type_roll'])
    dates = detect[detect > 0].index.to_list()

    # Get associated period types
    period_types = counts['type_roll'][detect > 0].to_list()

    # Create equivalent 'starting' lists that include the first period's type
    period_types_start = deepcopy(period_types)
    period_types_start.insert(0, counts['type_roll'].iloc[0])
    dates_start = deepcopy(dates)
    dates_start.insert(0, geod_neu_xy.index[0])

    # Add dummy values to make lists same size as _start variants
    period_types.append(0)
    dates.append(0)

    # Process each identified period of data in turn.
    store = []
    for d_st, d_en, s in zip(dates_start, dates, period_types_start):
        
        # Get the data from this period...
        if d_en == 0:
            pxyz = geod_neu_xy[geod_neu_xy.exclude == False][d_st:]
            d_en = geod_neu_xy[geod_neu_xy.exclude == False].index[-1]
        else:
            # pandas indexing is inclusive, so to prevent using the same data twice we
            # need to force the indexing to be 'exclusive'.
            d_en = d_en - pd.Timedelta(days=1)

            if s == 2:
                # Case of continuous data
                pxyz = geod_neu_xy[geod_neu_xy.exclude == False][d_st:d_en]
            elif s == 1:
                # Try to add a couple of days from the continuous time series, to improve rolling window smoothing.
                # This may need re-thinking!
                pxyz = geod_neu_xy[geod_neu_xy.exclude == False][d_st:d_en+pd.Timedelta(days=2)]
            elif s == 0:
                pass
            else:
                raise ValueError('Unknown occupation type.')

        message = 'Period {ds} to {de}: treating as {{type}} occupations.'.format(
                ds=d_st.strftime('%Y-%m-%d'),
                de=d_en.strftime('%Y-%m-%d'))

        # Now apply the correct type of filtering for the period type.
        if s == 1:
            # do daily processing
            print(message.format(type='daily'))
            #pxyz = process_daily_occups(pxyz)
        elif s == 2:
            # do continuous processing
            print(message.format(type='continuous'))
            pxyz = process_cont_occups(pxyz)
            occstr = 'continuous'
        elif s == 0:
            print(message.format(type='empty/ignore'))
            pass
        else:
            raise ValueError('Unknown occupation type.')

        # Make sure that we only save back data for the period,
        # without any data which may have been prepended/appended
        # to help with filtering edge effects.
        pxyz = pxyz[d_st:d_en]

        store.append(pxyz)

    filtd = pd.concat(store, axis=0)
    print('*** End of period-based processing ***')

# ## Smoothing the whole time series

if not args.stake:
    # Restore to original frequency and interpolate; adds a flag column named 'interpolated'
    print('Regularising whole series')
    filtd_i = pp.regularise(filtd, args.sample_freq)

    # Do Gaussian filtering - this df does not have interpolated column
    print('Gaussian filtering whole series')
    filtd_disp = pp.smooth_displacement(filtd_i, 7200*3) #7200secs ~ 2hours

    # Sub-set data to retain only original data samples (modified by Gaussian filtering)
    xyz = filtd_disp.filter(items=('x_m', 'y_m', 'z_m'), axis='columns')
    xyz = xyz[filtd_i.interpolated == 0]

# ## Calculate velocities

if not args.stake:
    # Calculate velocities
    print('Calculating velocities')
    v_24h = pp.calculate_period_velocities(filtd_disp['x_m'], '1D', tz=args.tz, method='epoch', sigmas=filtd.filter(items=['SigE_cm', 'SigN_cm'], axis='columns'))
    v_5d = pp.calculate_period_velocities(filtd_disp['x_m'], '5D', tz=args.tz, method='epoch', sigmas=filtd.filter(items=['SigE_cm', 'SigN_cm'], axis='columns'))
    v_15d = pp.calculate_period_velocities(filtd_disp['x_m'], '15D', tz=args.tz, method='epoch', sigmas=filtd.filter(items=['SigE_cm', 'SigN_cm'], axis='columns'))
    maxperday = pd.Timedelta('1D') / pd.Timedelta(args.sample_freq)
    dayperc = 100 / maxperday * filtd_i.interpolated[filtd_i.interpolated == 0].resample('1D').count()
    dayperc.name = 'obs_cover_percent'
    v_24h = pd.concat((v_24h, dayperc), axis=1)

    # Longer-term velocities, following Doyle 2014 approach
    # d6h = filtd_disp.resample('6h').mean()
    # p15d = d6h.resample('15D').first()
    # v15d = (p15d.x_m.shift(1) - p15d.x_m) * pp.v_mult('15D')

    # Regression-based approach
    # !!TODO!! implement a mix of Doyle for continuous and this for daily occs...
    # Want to apply this on the most raw data possible? i.e. not regliarsed and smoothed, as these
    # are not helpful operations on daily data
    # This doesn't really work on data filtered with a 'standard' approach - 
    # only currently works with geod_neu_xy. What would a good filtering strategy for
    # daily data look like?
    # Or do these need to be reprocessed by TRACK with ambiguities fixed?
    # p30d = filtd.x_m.resample('30D').apply(pp.position_by_regression)
    # v30d = (p30d.shift(1) - p30d) * pp.v_mult('30D')


# ## Export to disk

# +
# Define output filenames
output_to_pre = '{site}_{ys}_{ds}_{ye}_{de}'.format(
    site=args.site,
    ys=geod.index[0].year,
    ds=geod.index[0].timetuple().tm_yday,
    ye=geod.index[-1].year,
    de=geod.index[-1].timetuple().tm_yday
)

# Other outputs are appended '_disp' rather than '_geod'.
output_L2_base = os.path.join(path_output_L2, f'{output_to_pre}_disp')
output_L2_H5 =  '%s.h5' %output_L2_base

# Make sure that directory gets created, it may not exist yet
os.makedirs(path_output_L2, exist_ok=True)

# -

# If there is an existing file, delete it to prevent conflicts.
if os.path.exists(output_L2_H5):
    os.remove(output_L2_H5)
    print('Old main output file found, deleted.')

# ### Save displacements

# Save displacements to disk
xyz.to_hdf(output_L2_H5, key='xyz', format='table')
print('Saved displacements.')
print('Main output file: %s ' %output_L2_H5)

# ### Save velocities

if not args.stake:
    v_24h.to_hdf(output_L2_H5, key='v_24h', format='table')
    v_5d.to_hdf(output_L2_H5, key='v_5d', format='table')
    v_15d.to_hdf(output_L2_H5, key='v_15d', format='table')
    
    v_24h.to_csv('%s_velocity_24h.csv' %output_L2_base)
    v_5d.to_csv('%s_velocity_5d.csv' %output_L2_base)
    v_15d.to_csv('%s_velocity_15d.csv' %output_L2_base)
       
    if args.v6h:
        v_6h = pp.calculate_short_velocities(filtd_disp['x'], '6h')
        v_6h.to_hdf(output_L2_H5, key='v_6h', format='table')


# ## Diagnostic plots of final outputs

if not args.noplot:
    try:
        import seaborn as sns
        sns.set_style('whitegrid')
    except:
        pass

# ### For stakes

if not args.noplot and args.stake:
    plt.figure()
    plt.plot(geod_neu_xy.x_m, geod_neu_xy.y_m, '.', color='gray', alpha=0.3, label='GEOD (after exclusions)')
    plt.title('Stake/Quick-Pos X-Y')
    plt.savefig('%s_xy.png' %output_L2_base, dpi=300)

# ### For GPS stations

# +
label_geod = 'All epochs (Level-1 GEOD)'
label_xyz = 'Retained epochs (Level-2 XYZ)'
label_iterp = 'Filt./Smooth./Interp. (for L-2 Vel.)'

if not args.noplot and not args.stake:
    do_plot = True
else:
    do_plot = False
# -

# Plot X versus Y
if do_plot:
    plt.figure()
    plt.plot(geod_neu_xy.x_m, geod_neu_xy.y_m, '.', color='gray', alpha=0.3, label=label_geod)
    plt.plot(xyz.x_m, xyz.y_m, '.', color='tab:purple', alpha=0.3, label=label_xyz)
    plt.plot(filtd.x_m, filtd.y_m, '-', color='tab:blue', alpha=0.5, label=label_iterp)
    plt.xlabel('Metres')
    plt.ylabel('Metres')
    plt.title('%s X - Y' %args.site)
    plt.legend()
    plt.savefig('%s_xy.png' %output_L2_base, dpi=300)

# Plot Time versus X
if do_plot:
    plt.figure()
    if os.path.exists(exclusions_file):
        excl = pd.read_csv(exclusions_file, parse_dates=['excl_start', 'excl_end'])
        for ix, row in excl.iterrows():
            plt.axvspan(row.excl_start, row.excl_end, alpha=0.1, color='tab:red')
    plt.plot(geod_neu_xy.index, geod_neu_xy.x_m, '.', color='gray', alpha=0.3, label=label_geod)
    plt.plot(xyz.index, xyz.x_m, '.', color='tab:purple', alpha=0.3, label=label_xyz)
    plt.plot(filtd_disp.index, filtd_disp.x_m, '-', color='tab:blue', alpha=0.5, label=label_iterp)
    plt.ylabel('Metres')
    plt.title('%s X - Time' %args.site)
    plt.legend()
    plt.savefig('%s_xt.png' %output_L2_base, dpi=300)

# Plot Y versus Time
if do_plot:
    plt.figure()
    plt.plot(geod_neu_xy.y_m, geod_neu_xy.index, '.', color='gray', alpha=0.3, label=label_geod)
    plt.plot(xyz.y_m, xyz.index, '.', color='tab:purple', alpha=0.3, label=label_xyz)
    plt.plot(filtd_disp.y_m, filtd_disp.index, '-', color='tab:blue', alpha=0.5, label=label_iterp)
    plt.xlabel('Metres')
    plt.title('%s Time - Y' %args.site) 
    plt.legend()
    plt.savefig('%s_ty.png' %output_L2_base, dpi=300)

# Plot Time versus Z
if do_plot:
    plt.figure()
    plt.plot(geod_neu_xy.index, geod_neu_xy.z_m, '.', color='gray', alpha=0.3, label=label_geod)
    plt.plot(xyz.index, xyz.z_m, '.', color='tab:purple', alpha=0.3, label=label_xyz)
    plt.plot(filtd_disp.index, filtd_disp.z_m, '-', color='tab:blue', alpha=0.5, label=label_iterp)
    plt.ylabel('Metres')
    plt.title('%s Time - Z' %args.site) 
    plt.legend()
    plt.savefig('%s_tz.png' %output_L2_base, dpi=300)

# Plot 24-hour differenced velocities
if do_plot:
    plt.figure()
    plt.errorbar(v_24h.index+pd.Timedelta(hours=12), v_24h.v_myr, yerr=v_24h.unc_myr, elinewidth=0.2, ecolor='tab:blue', drawstyle='steps-mid', alpha=0.5)
    plt.errorbar(v_5d.index+pd.Timedelta(hours=(5*24)/2), v_5d.v_myr, yerr=v_5d.unc_myr, elinewidth=1, color='k', ecolor='k', drawstyle='steps-mid', linewidth=2)
    plt.errorbar(v_15d.index+pd.Timedelta(hours=(15*24)/2), v_15d.v_myr, yerr=v_15d.unc_myr, elinewidth=1, color='tab:red', ecolor='tab:red', drawstyle='steps-mid', linewidth=2)
    plt.ylim(0, v_24h.v_myr.max()+10)
    #plt.figure()
    #ax1 = plt.subplot(211)
    #v_24h.v_myr.plot(ax=ax1, alpha=0.5)
    # Use steps-pre: velocity calculation for day0 is (X_day1 - Xday0), so the step will show what happens that day.
    #v_24h.v_myr.plot(drawstyle='steps-pre', ax=ax1)
    plt.title('%s 24-H, 5-D and 15-D velocity' %args.site)
    plt.ylabel('m/yr')
    #ax2 = plt.subplot(212, sharex=ax1)
    #v_24h.obs_cover_percent.plot(drawstyle='steps-pre', ax=ax2)
    #plt.ylabel(r'% daily obs cover')
    plt.savefig('%s_v24h_5d.png' %output_L2_base, dpi=300)

# Summer-only plots
if do_plot:
    ystart = v_24h.index.year.unique().min()
    yend = v_24h.index.year.unique().max()
    for year in range(ystart, yend):
        
        v_24h_here = v_24h.loc[f'{year}-05-01':f'{year}-10-01']
        print(year, v_24h_here.obs_cover_percent.mean())
        if v_24h_here.obs_cover_percent.mean() <= 0.1:
            continue
        
        plt.figure()
        v_5d_here = v_5d.loc[f'{year}-05-01':f'{year}-10-01']
        v_15d_here = v_15d.loc[f'{year}-05-01':f'{year}-10-01']
        plt.errorbar(v_24h_here.index+pd.Timedelta(hours=12), v_24h_here.v_myr, yerr=v_24h_here.unc_myr, elinewidth=0.2, ecolor='tab:blue', drawstyle='steps-mid', alpha=0.5)
        plt.errorbar(v_5d_here.index+pd.Timedelta(hours=(5*24)/2), v_5d_here.v_myr, yerr=v_5d_here.unc_myr, elinewidth=1, color='k', ecolor='k', drawstyle='steps-mid', linewidth=2)
        plt.errorbar(v_15d_here.index+pd.Timedelta(hours=(15*24)/2), v_15d_here.v_myr, yerr=v_15d_here.unc_myr, elinewidth=1, color='tab:red', ecolor='tab:red', drawstyle='steps-mid', linewidth=2)
        plt.xlim(pd.Timestamp(year, 5, 1), pd.Timestamp(year, 10, 1))
        plt.ylim(0, v_24h_here.v_myr.max()+20)
        plt.title('%s 24-H, 5-D and 15-D velocity, Summer %s' %(args.site, year))
        plt.ylabel('m/yr')
        #plt.savefig('%s_v24h_5d_summer_%s.png' %(output_L2_base, year), dpi=300)

# +
# Plot velocities over other window lengths
# needs uncertainties!
# if do_plot:       
#     plt.figure()
#     v15d.plot(drawstyle='steps-pre', label='15d')
#     v30d.plot(drawstyle='steps-pre', label='30d')
#     plt.title('%s 15/30 Day velocity' %args.site)
#     plt.ylabel('m/yr')
#     plt.legend()
#     plt.savefig('%s_v15d.png' %output_L2_base, dpi=300)

# +
# Plot instantaneous 6-hour differenced velocities
# if do_plot:       
#     plt.figure()
#     v_6h.plot()
#     plt.title('%s Instantaneous 6H velocity' %args.site)
#     plt.ylabel('m/yr')
#     plt.savefig('%s_v6h.png' %output_L2_base, dpi=300)
# -

# ## Development feature: subset the parquet file to a smaller file
# Use this to save a smaller dataset which you can use with this Notebook during testing of filtering procedures. This saves a dataset which contains only unmodified data, i.e. no filtering has been applied to it!

# +
# if is_notebook():
#     # Put in your dates of interest here
#     smaller_data = geod.loc['2021-10-01':'2021-12-31']
#     # Then let's work out the correct filename
#     smaller_fn = '{site}_{ys}_{ds}_{ye}_{de}.parquet'.format(
#         site=args.site,
#         ys=smaller_data.index[0].year,
#         ds=smaller_data.index[0].timetuple().tm_yday,
#         ye=smaller_data.index[-1].year,
#         de=smaller_data.index[-1].timetuple().tm_yday
#     )
#     smaller_data.to_parquet(smaller_fn)
#     print('Output to %s.' %smaller_fn)
# -

# ## Under development: calculating velocities over flexible window lengths

# +
# ### Try a flexible window length approach on raw data.
# filt_manual = pp.filter_positions(geod_neu_xy)
# # Doyle et al use 6H positions
# # Can I use regression across several days to estimate the accuracy/quality of the single-day-obs used?
# fmanres = filt_manual.x[(filt_manual.index.hour >= 9) & (filt_manual.index.hour <= 15)].resample('1D').mean()

# +
# #fmanres['2021-12-04'] = np.nan
# #fmanres['2021-12-05'] = np.nan
# ## Velocity over a minimum window length, looking ahead for the first position value at least x days away from current obs, otherwise more.
# # need to start with a 'good' date, how best to identify this?
# procdate = fmanres.index[0] #pd.Timestamp('2021-05-10')
# store = []
# while True:
#     loc_today = fmanres.loc[procdate]
#     loc_next = fmanres.loc[procdate + pd.Timedelta(days=15):]
#     loc_next = loc_next.dropna()
#     if len(loc_next) == 0:
#         print('End of series')
#         break
#     date_next = loc_next.index[0]
#     loc_next = loc_next.iloc[0]
#     print(loc_next)
#     tdiff = date_next - procdate
#     diff = np.abs(loc_next - loc_today)
#     vel = diff * pp.v_mult('%sD'%tdiff.days)
#     store.append(dict(start=procdate, finish=date_next, disp=diff, velocity=vel))
#     procdate = date_next
# outp = pd.DataFrame(store)
# +
# import statsmodels.api as sm
# def detrend(df):
#     X = sm.add_constant(df.index.to_julian_date())
#     y = df['x']
#     rz = sm.OLS(y, X).fit()
#     slope = rz.params['const']
#     detr = df['x'] - (rz.params['x1']*df.index.to_julian_date() + rz.params['const'])
#     return detr

# store = {}
# for date in pd.date_range('2023-07-01','2023-09-01'):
#     store[date] = detrend(geod_neu_xy.loc[date-pd.Timedelta(hours=1):date+pd.Timedelta(hours=1)]).std()
# uncs = pd.Series(store)

# def unc(df):
#     #print(df.index[0], df.index[-1])
#     if len(df) > 1:
#         return df.loc[:df.index[0]+pd.Timedelta(hours=3)].mean()
#     else:
#         return np.nan
# uncs_e = filtd['SigE_cm'].resample('1D', offset='-3h').apply(unc).shift(freq='3h') * 0.01
# uncs_n = filtd['SigN_cm'].resample('1D', offset='-3h').apply(unc).shift(freq='3h') * 0.01
# uncs = np.sqrt(uncs_e**2 + uncs_n**2)
# v_24h_new = pp.calculate_daily_velocities(filtd_disp['x'], tz=args.tz, method='epoch', epoch_uncertainty=uncs)

# uncs_e = filtd['SigE'].resample('5D', offset='-3h').apply(unc).shift(freq='3h') * 0.01
# uncs_n = filtd['SigN'].resample('5D', offset='-3h').apply(unc).shift(freq='3h') * 0.01
# uncs = np.sqrt(uncs_e**2 + uncs_n**2)
# v_5d = pp.calculate_daily_velocities(filtd_disp['x'], tz=args.tz, method='epoch', epoch_uncertainty=uncs, win='5D')
