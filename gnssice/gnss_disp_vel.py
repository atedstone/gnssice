# # gnss_disp_vel.py
#
# Process TRACK-corrected GNSS data into displacement and velocity time series:
# - Filter bad data
# - Median filtering to remove outliers
# - Gaussian filtering to smooth series
# - Differencing to derive velocities
# - Export to disk
#
# This script looks for Level-1 inputs in `$GNSS_L1DIR/site/` and stores outputs to
# `$GNSS_L2DIR/site`. By default it will look for the origin, rotation and exclusion 
# files in `$GNSS_L2DIR/site/`.
#
# ## USAGE
# On command line:
#
#     python gnss_disp_vel.py -h
#
# To get help on all available options.
#
# In ipython:
#
#     # # # # # # # # # # # %run /path/to/gnss_disp_vel.py -h
#     
# As a Notebook:
#
#     In JupyterLab's File Browser pane, right-click this file, then select Open With -> Notebook.
#
#     Insert your arguments in the section 'Running in a Notebook? Put in your parameters below!'
#     
#
# ## DATA INPUTS
#
# Mandatory:
# - parquet file (generated by `export_level1.py`)
# - `origin_<site>.csv` (generated by `calculate_local_origin.py`)
# - Filter parameters file (by default, uses `level2_parameter_defaults.yaml` found in package directory)
#
# Generated if not available:
# - `rotation_<site>.dat`
#
# Optional:
# - `exclusions_<site>.csv`
# - `pole_corrections_<site>.csv`
# - Temporal filter parameters adjustment file, provide filename using `-tf` argument.
#
# ## OUTPUTS 
# - h5 file
# - csv file with v_24h.
#
# ## HISTORY
# - Based on `plot_site_rel_2009_2012_geod.m`, developed by Matt King, Andrew Sole, Ian Bartholomew, Andrew Tedstone during 2009-2012.
# - Created 2022-07-27 AJT.
# - Option to run as Notebook introduced March 2023, AJT.
# - Updates and improvements including to pp.py July/August 2025, AJT.
# - New epoch-by-epoch velocities calculations, Nov. 2025, AJT.
# - Moved filter parameters to YAML files, Dec. 2025, AJT.
#
#

import pandas as pd
import os
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.ticker import FuncFormatter
from mpl_toolkits.mplot3d import Axes3D
import argparse
from scipy.stats import mode
from copy import deepcopy
from glob import glob
import yaml
import re

from gnssice import pp

# ## Input parameters

# +
p = argparse.ArgumentParser('Kinematic GPS: filter positions, calculate \
    trajectories, calculate velocities. Output .h5 files of displacements, \
    CSV file of longer-term velocities, plots of data.')
p.add_argument('site', type=str, help='Name/identifier of site')
p.add_argument('-f', type=str, default=None, help='Location of YAML Filter file containing default filter parameters. \
    Defaults to `level2_parameter_defaults.yaml` found in installation directory.')
p.add_argument('-tf', type=str, default=None, help='Location of YAML Temporal Filter file containing filter parameters for specific temporal periods.')
p.add_argument('-legacy', action='store_true', help='Output velocities calculated using "legacy" (Edin. 2009-13) approach.')
p.add_argument('-optspath', type=str, default=None, help='Location of options files (origin, exclusions...) if \
    not $GNSS_L2DIR')
p.add_argument('-noexcl', action='store_true', help='Disable excluding user-specified periods based on exclusion CSV file.')
p.add_argument('-nocorrect', action='store_true', help='Disable apply pole corrections based on corrections CSV file.')
p.add_argument('-noplot', action='store_true', help='Do not generate plots.')
p.add_argument('-stake', action='store_true', help='Short-occupation stake mode. \
    Do not apply any filtering or smoothing operations. This is useful for \
    processing short-occupation fixes.')
p.add_argument('-tz', type=str, help='Localise v_24h to the specified timezone.')
p.add_argument('-sample_freq', type=str, default='10s')
p.add_argument('-v6h', action='store_true', help='Compute sliding 6h velocities for the site.')

p.add_argument('-nbplot', type=str, default='widget', help='If running as a Notebook, \
               defines the matplotlib backend used. Supply widget or inline.')
print('')
# -

# ### Running in a Notebook? Put your parameters in below!

# +
# If running as a Notebook, provide your arguments to the ArgumentParser here.
# These arguments are only read when running as a Notebook, otherwise they are ignored.

# Examples:

# A continous site with no extra options:
# input_args = ['lev5']
# A stake only site:
# input_args = ['f004', '-stake']

# A site where you also want the 6-hour velocities:
# input_args = ['lev5', '-v6h']

# Output both epoch (new-style) and legacy (old-style) velocities, without pole corrections
# input_args = ['lev5', '-legacy', '-nocorrect']

# Arguments with additional inputs
# input_args = ['lev5', '-tf', 'path/to/my_file.yaml', '-legacy']

#input_args = ['lev5', '-tf', '/Users/atedston/scripts/gnssice/gnssice/level2_temporal_filter_example.yaml', '-legacy']
input_args = ['kanu', '-legacy']


# -

# ## Identify execution mode

# +
def is_notebook() -> bool:
    """stackoverflow.com/"""
    try:
        shell = get_ipython().__class__.__name__
        print('shell:', shell)
        if shell == 'ZMQInteractiveShell': 
            return True
        elif shell == 'TerminalInteractiveShell':
            return False
        else:
            return False
    except NameError:
        return False

if is_notebook(): 
    print('Notebook mode')
    args = p.parse_args(input_args)
    if args.nbplot == 'widget':
        print('widget')
        # %matplotlib widget
    elif args.nbplot == 'inline':
        print('inline')
        # %matplotlib inline
    else:
        raise ValueError('Unknown matplotlib backend specified for Notebook. Only widget or inline are understood (provided %s)' %args.nbplot)
    # %load_ext autoreload
    # %autoreload 2
    # %aimport gnssice
else:
    print('Terminal mode')
    args = p.parse_args()


# +
path_output_L2 = os.path.join(os.environ['GNSS_L2DIR'], args.site)
#Path(path_output_L2).mkdir(exist_ok=True)
print(f'Level-2 Directory: {path_output_L2}')

# By default, find the options files (e.g. exclusions) in the Level-2 directory.
if args.optspath is None:
    args.optspath = path_output_L2


# -

# ## Load configuration/options

# +
def read_yaml(fn):
    with open(fn) as cf_file:
        pfc = yaml.safe_load( cf_file.read() )
        return pfc
    
# Read the file containing the filter defaults
if args.f is None:
    fn = Path(os.path.abspath(pp.__file__)).parent / 'level2_parameter_defaults.yaml'
else:
    fn = args.f
config = read_yaml(fn)

# And also read the temporal filter file, if one is provided
if args.tf is not None:
    pfc = read_yaml(args.tf)
else:
    pfc = None
# -

# ## Load Level-1 data

# +
#level1_path = os.path.join(os.environ['GNSS_L1DIR'], args.site, '*_geod.parquet')
# level1_path = os.path.join(os.environ['GNSS_L1DIR'], args.site, '*_geod.parquet')
# f = glob(level1_path)
# if len(f) == 0:
#     raise FileNotFoundError(f'No level-1 file found for {args.site}.')
# elif len(f) > 1:
#     raise ValueError(f'More than one level-1 file found for {args.site}.')
# else:
#     f = f[0]

level1_path = os.path.join(os.environ['GNSS_L1DIR'], args.site)
res = [f for f in os.listdir(level1_path) if re.match(pp.REGEX_L1_FILE, f)]
for f in res:
    print(f)
    
if len(res) > 1:
    raise ValueError('More than one 24h velocity file found.')
elif len(res) == 0:
    raise ValueError('No v24h file found.')
else:
    f = res[0]
    
print(f'Loading Level-1 file: {f}...')

geod = pd.read_parquet(os.path.join(level1_path, f.strip()))
geod = pp.update_legacy_geod_col_names(geod)
# -

geod.columns

# ## Convert coordinates to local north-east-up
# This section calculates coordinates in metres relative to the installation origin of the site.

# Load coordinates of GPS origin point (basically its installation location)
try:
    origin_file = os.path.join(args.optspath, 'origin_%s.csv' %args.site)
    origin = pd.read_csv(origin_file)
except IOError:
    raise IOError('origin_<site>.csv file mandatory but not found.')
origin = origin.iloc[0]

# Calculate North-East-Up
print('Converting to local North-East-Up')
neu = pp.calculate_local_neu(geod, 
    origin['x0'], origin['y0'], origin['z0'], 
    origin['lat0'], origin['lon0'])

# Concatenate results to df
geod_neu = pd.concat((geod, neu), axis=1)

# ## Rotate to along-track (x) and across-track (y) displacement

rfile = os.path.join(args.optspath, 'rotation_%s.dat' %args.site)
if os.path.exists(rfile):
    print('Loading existing rotation matrix')
    r1 = np.loadtxt(rfile)
else:
    print('No rotation matrix file for site exists, creating using entire contents of GEOD file')
    directions = pp.calculate_displacement_trajectory(geod_neu)
    r1 = pp.create_rot_matrix(directions)
    np.savetxt(rfile, r1)

print('Doing rotation')
xy = pp.rotate_to_displacements(geod_neu['East_m'], geod_neu['North_m'], r1)
geod_neu_xy = pd.concat((geod_neu, xy), axis=1)

# ## Apply user pole corrections

# These corrections are based on already-rotated coordinates, NOT on North-East-Up
corrections_file = os.path.join(args.optspath, 'pole_corrections_%s.csv' %args.site)
if not args.nocorrect and os.path.exists(corrections_file):
    print('Applying pole corrections')
    geod_neu_xy = pp.correct_pole_changes(geod_neu_xy, corrections_file)

# ## Apply user exclusions

exclusions_file = os.path.join(args.optspath, 'exclusions_%s.csv' %args.site)
if not args.noexcl and os.path.exists(exclusions_file):
    print('Applying exclusions')
    geod_neu_xy = pp.apply_exclusions(geod_neu_xy, exclusions_file)
else:
    geod_neu_xy['exclude'] = False

# ## Apply pre-filtering to each period of data
# Here, we primarily remove 'bad' data according to TRACK outputs, and apply median-based-filtering.

# ### Set up filter options per-period

# +
# Set up a Series to record the filter options set ID for each epoch.
filter_opts = pd.Series(0, geod_neu_xy.index)

# Keys that must be provided for each period.
mandatory_keys = ['date_start', 'date_finish']
# This functionality only supports filter options for...
supported_keys = ['filter_positions_episodic', 'filter_positions_continuous', 'remove_displacement_outliers']

# Set up a store to save each customised filter set to
# The default set has ID=0
config_sets = {
    0: config
}

# Subsequent filter sets begin at ID=1.
i = 1
if pfc is not None: 
    # Iterate through each customised period defined in the YAML file.
    for period in pfc['periods']:
    
        # Take a copy of the default config set to modify
        config_here = deepcopy(config)
    
        # Check that this period has the mandatory keys needed to build the custom filter.
        for mk in mandatory_keys:
            if mk not in period.keys():
                raise ValueError(f'Parameter `{mk}` missing from period filter specification.')
            config_here[mk] = period[mk]
    
        # Check that only supported filters are being modified.
        for key in period.keys():
            if key not in (mandatory_keys + supported_keys):
                raise ValueError(f'Unsupported filter found (supported filters: {supported_keys})')

        # Build the customised filter set for this period.
        for key in supported_keys:
            if key in period:
                for subkey in period[key]:
                    config_here[key][subkey] = period[key][subkey]

        # Save the filter set.
        config_sets[i] = config_here
        # Save the corresponding ID into the filter options time series.
        filter_opts.loc[period['date_start']:period['date_finish']] = i
        i += 1

# Identify the distinct time periods for each filter options set
detect = np.abs(filter_opts.shift(1) - filter_opts)
dates = detect[detect > 0].index.to_list()

dates_start = deepcopy(dates)
dates_start.insert(0, geod_neu_xy.index[0])

# Filter configuration set for each time period
filter_ids = filter_opts[dates_start]

dates.append(geod_neu_xy.index[-1])

# Date pairs
filter_periods = list(zip(dates_start, dates, filter_ids))
# -

# ### Run the filtering

if args.stake:
    xyz = geod_neu_xy[geod_neu_xy.exclude == False].filter(items=('x_m', 'y_m', 'z_m'), axis='columns')

# +
if not args.stake:

    store = []
    
    # Iterate through each distinct filter period.
    for fp in filter_periods:
        fcfg = config_sets[fp[2]]
        data_here = geod_neu_xy.loc[fp[0]:fp[1]]

        print('')
        print('FILTER PARAMETERS PERIOD: {0}-{1}'.format(fp[0], fp[1]))
        print(fcfg)
        
        ## Identify periods of (i) continuous occupations, (ii) daily occupations, (iii) no occupation
    
        def occ_type(d, threshold_continuous=1000):
            """ Identify and label occupation type for each data period.
            
            This function distinguishes between 'episodic'/'daily' 
            occupation and 'continuous' occupation by means of the number
            of observations made every day. If there are more observations
            than the threshold then the site is considered to be occupied 
            continuously on this day.
    
            value of 2 = continuous (e.g. 10 sec) occupation
                     1 = 'episodic', e.g. for an hour
                     0 = no occupation
    
            :param d: data value
            :param threshold_continuous: number of raw observations above which the site
                can be considered to be recording continuously.
            :returns: 0, 1 or 2
            """
            if d > threshold_continuous: return 2
            if d < threshold_continuous and d > 0: return 1
            return 0
    
        def process_daily_occups(df):
            """ The filtering processes for episodic/daily observations. """
            # Find the modal hour of this period's observations
            df = pp.filter_positions(df, 
                                     fcfg['filter_positions_episodic']['rms'],
                                     fcfg['filter_positions_episodic']['h'],
                                     fcfg['filter_positions_episodic']['N'], 
                                     fcfg['filter_positions_episodic']['NotF']
                                    )
            #occ_time = '%sH' %int(mode(df.index.hour)[0])
            # Use this modal hour to place the index of the mean positions at day:hour.
            #df = df.resample('1D', offset=occ_time).mean().rolling('10D', center=True).mean()
            return df
    
        def process_cont_occups(df):
            """ The filtering processes for continuous observations. """
            # Do initial filtering
            print('Filtering out bad positions (according to RMS etc)')
            n = len(df)
            print(f'\t Starting with {n} positions')
            df = pp.filter_positions(df, 
                         fcfg['filter_positions_continuous']['rms'],
                         fcfg['filter_positions_continuous']['h'],
                         fcfg['filter_positions_continuous']['N'], 
                         fcfg['filter_positions_continuous']['NotF']
                        )

            nn = len(df)
            p = np.round(100 / n * nn, 0)
            print(f'\t {nn} ({p} %) left after applying filter criteria')
            if nn == 0:
                return None

            
                
            df = pp.remove_displacement_outliers(df, 
                                                 args.sample_freq,
                                                 fcfg['remove_displacement_outliers']['diff_x_m'],
                                                 fcfg['remove_displacement_outliers']['diff_y_m'],
                                                 fcfg['remove_displacement_outliers']['diff_z_m'],
                                                 fcfg['remove_displacement_outliers']['median_win'],
                                                 fcfg['remove_displacement_outliers']['sigma_mult'],
                                                 iterations=fcfg['remove_displacement_outliers'].get('iterations', 2)
                                                )
            nnn = len(df)
            p = np.round(100 / n * nnn, 0)
            print(f'\t {nnn} ({p} %)left after removing displacement outliers')
            if nnn == 0:
                return None
                
            return df
    
        # Count number of daily observations
        counts = data_here[data_here.exclude == False].x_m.resample('1D').count().to_frame()
        counts['type'] = counts['x_m'].apply(occ_type)
        counts['type_roll'] = counts['type'].rolling('3D', center=True).max() #apply(lambda x: mode(x)[0])
    
        # Find change dates
        detect = np.abs(counts['type_roll'].shift(1) - counts['type_roll'])
        dates = detect[detect > 0].index.to_list()
    
        # Get associated period types
        period_types = counts['type_roll'][detect > 0].to_list()
    
        # Create equivalent 'starting' lists that include the first period's type
        period_types_start = deepcopy(period_types)
        period_types_start.insert(0, counts['type_roll'].iloc[0])
        dates_start = deepcopy(dates)
        dates_start.insert(0, data_here.index[0])
    
        # Add dummy values to make lists same size as _start variants
        period_types.append(0)
        dates.append(0)
    
        # Process each identified period of data in turn.
        for d_st, d_en, s in zip(dates_start, dates, period_types_start):
            
            # Get the data from this period...
            if d_en == 0:
                pxyz = data_here[data_here.exclude == False][d_st:]
                d_en = data_here[data_here.exclude == False].index[-1]
            else:
                # pandas indexing is inclusive, so to prevent using the same data twice we
                # need to force the indexing to be 'exclusive'.
                d_en = d_en - pd.Timedelta(days=1)
    
                if s == 2:
                    # Case of continuous data
                    pxyz = data_here[data_here.exclude == False][d_st:d_en]
                elif s == 1:
                    # Try to add a couple of days from the continuous time series, to improve rolling window smoothing.
                    # This may need re-thinking!
                    pxyz = data_here[data_here.exclude == False][d_st:d_en+pd.Timedelta(days=2)]
                elif s == 0:
                    pass
                else:
                    raise ValueError('Unknown occupation type.')
    
            message = 'Period {ds} to {de}: treating as {{type}} occupations.'.format(
                    ds=d_st.strftime('%Y-%m-%d'),
                    de=d_en.strftime('%Y-%m-%d'))
    
            # Now apply the correct type of filtering for the period type.
            if s == 1:
                # do daily processing
                print(message.format(type='daily'))
                #pxyz = process_daily_occups(pxyz)
            elif s == 2:
                # do continuous processing
                print(message.format(type='continuous'))
                pxyz = process_cont_occups(pxyz)
                occstr = 'continuous'
            elif s == 0:
                print(message.format(type='empty/ignore'))
                pass
            else:
                raise ValueError('Unknown occupation type.')

            if pxyz is not None:
                # Make sure that we only save back data for the period,
                # without any data which may have been prepended/appended
                # to help with filtering edge effects.
                pxyz = pxyz[d_st:d_en]
        
                store.append(pxyz)
    
filtd = pd.concat(store, axis=0)
print('*** End of period-based processing ***')
# -

filtd.drop_duplicates()

# ## Smoothing the whole time series

if not args.stake:
    # check for duplicates, one or two epochs often introduced if using temporally varying window
    n = len(filtd)
    filtd = filtd.drop_duplicates()
    diff = n - len(filtd)
    print(f'Removed {diff} duplicates')
    
    # Restore to original frequency and interpolate; adds a flag column named 'interpolated'
    print('Regularising whole series')
    filtd_i = pp.regularise(filtd, args.sample_freq)

    # Do Gaussian filtering - this df does not have interpolated column
    print('Gaussian filtering whole series')
    filtd_disp = pp.smooth_displacement(filtd_i, 
                                        config['smooth_displacement']['gauss_win_secs'], 
                                        gauss_win_z_mult=config['smooth_displacement']['gauss_win_z_mult']
                                       )

    # Sub-set data to retain only original data samples (modified by Gaussian filtering)
    xyz = filtd_disp.filter(items=('x_m', 'y_m', 'z_m'), axis='columns')
    xyz['interpolated'] = filtd_i['interpolated'].astype(bool)
    #xyz = xyz[filtd_i.interpolated == 0]

# ## Calculate velocities

if not args.stake:
    print('Calculating epoch-to-epoch velocities')
    # Supply the smoothed and interpolated x values, but the unsmoothed, uninterpolated sigma values.
    # Convert TRACK cm sigmas to metres
    v24h_epoch = pp.calculate_epoch_velocities_and_uncertainties(filtd_disp['x_m'], filtd['SigE_cm']*0.01, filtd['SigN_cm']*0.01, '1D')#
    v5d_epoch = pp.calculate_epoch_velocities_and_uncertainties(filtd_disp['x_m'], filtd['SigE_cm']*0.01, filtd['SigN_cm']*0.01, '5D')
    v15d_epoch = pp.calculate_epoch_velocities_and_uncertainties(filtd_disp['x_m'], filtd['SigE_cm']*0.01, filtd['SigN_cm']*0.01, '15D')

if not args.stake and args.legacy:
    # Calculate velocities
    print('Calculating regularised velocities (legacy approach)')
    # 'regular' velocities in the sense of a regular/constant/fixed time interval. Note that underlying observations may not be available at these intervals.
    v24h_reg = pp.calculate_regular_velocities(filtd_disp['x_m'], '1D', tz=args.tz, method='epoch', sigmas=(filtd['SigE_cm']*0.01, filtd['SigN_cm'] * 0.01))
    v5d_reg = pp.calculate_regular_velocities(filtd_disp['x_m'], '5D', tz=args.tz, method='epoch', sigmas=(filtd['SigE_cm']*0.01, filtd['SigN_cm'] * 0.01))
    v15d_reg = pp.calculate_regular_velocities(filtd_disp['x_m'], '15D', tz=args.tz, method='epoch', sigmas=(filtd['SigE_cm']*0.01, filtd['SigN_cm'] * 0.01))
    maxperday = pd.Timedelta('1D') / pd.Timedelta(args.sample_freq)
    dayperc = 100 / maxperday * filtd_i.interpolated[filtd_i.interpolated == 0].resample('1D').count()
    dayperc.name = 'obs_cover_percent'
    v24h_reg = pd.concat((v24h_reg, dayperc), axis=1)

# ## Export to disk

# +
# Define output filenames
output_to_pre = '{site}_{ys}_{ds}_{ye}_{de}'.format(
    site=args.site,
    ys=geod.index[0].year,
    ds=geod.index[0].timetuple().tm_yday,
    ye=geod.index[-1].year,
    de=geod.index[-1].timetuple().tm_yday
)

# Other outputs are appended '_disp' rather than '_geod'.
output_L2_base = os.path.join(path_output_L2, f'{output_to_pre}_disp')
output_L2_H5 =  '%s.h5' %output_L2_base

# Make sure that directory gets created, it may not exist yet
os.makedirs(path_output_L2, exist_ok=True)

# -

# If there is an existing file, delete it to prevent conflicts.
if os.path.exists(output_L2_H5):
    os.remove(output_L2_H5)
    print('Old main output file found, deleted.')

# ### Save filters info

ffile = output_L2_base + '_filter_parameters.yaml'
with open(ffile, 'w+') as fh:
    fh.write('# Record of the level-2 filtering parameters used to generate this Level-2 dataset. \n')
    fh.write('# This is a reference-only file, generated from the defaults YAML and any user-specific YAML. \n')
    fh.write('# The default filter parameters are found in position 0. \n')
    fh.write('# Any periods in which different filter parameters were set are listed starting from position 1. \n')
    yaml.dump(config_sets, fh)

# ### Save displacements

# Save displacements to disk
xyz.to_hdf(output_L2_H5, key='xyz', format='table')
print('Saved displacements.')
print('Main output file: %s ' %output_L2_H5)

# ### Save velocities

if not args.stake:

    v24h_epoch.to_hdf(output_L2_H5, key='v_24h_epochs', format='table')
    v5d_epoch.to_hdf(output_L2_H5, key='v_5d_epochs', format='table')
    v15d_epoch.to_hdf(output_L2_H5, key='v_15d_epochs', format='table')
    v24h_epoch.to_csv('%s_velocity_24h_epochs.csv' %output_L2_base)
    v5d_epoch.to_csv('%s_velocity_5d_epochs.csv' %output_L2_base)
    v15d_epoch.to_csv('%s_velocity_15d_epochs.csv' %output_L2_base)

    if args.legacy:
        v24h_reg.to_hdf(output_L2_H5, key='v_24h_legacy', format='table')
        v5d_reg.to_hdf(output_L2_H5, key='v_5d_legacy', format='table')
        v15d_reg.to_hdf(output_L2_H5, key='v_15d_legacy', format='table')
        v24h_reg.to_csv('%s_velocity_24h_legacy.csv' %output_L2_base)
        v5d_reg.to_csv('%s_velocity_5d_legacy.csv' %output_L2_base)
        v15d_reg.to_csv('%s_velocity_15d_legacy.csv' %output_L2_base)
           
    if args.v6h:
        v_6h = pp.calculate_short_velocities(filtd_disp['x'], '6h')
        v_6h.to_hdf(output_L2_H5, key='v_6h', format='table')


# ## Diagnostic plots of final outputs

if not args.noplot:
    try:
        import seaborn as sns
        sns.set_style('whitegrid')
    except:
        pass

# ### For stakes

if not args.noplot and args.stake:
    plt.figure()
    plt.plot(geod_neu_xy.x_m, geod_neu_xy.y_m, '.', color='gray', alpha=0.3, label='GEOD (after exclusions)')
    plt.title('Stake/Quick-Pos X-Y')
    plt.savefig('%s_xy.png' %output_L2_base, dpi=300)

# ### For GPS stations

# +
label_geod = 'All epochs (Level-1 GEOD)'
label_xyz = 'Retained epochs (Level-2 XYZ)'
label_iterp = 'Filt./Smooth./Interp. (for L-2 Vel.)'

if not args.noplot and not args.stake:
    do_plot = True
else:
    do_plot = False


# -

# #### Positions

# Function defining axis limits
def ax_lim(data,scl_fac):
    mn = np.nanmean(data)
    stdv = np.nanstd(data)
    return(mn - scl_fac * stdv, mn + scl_fac * stdv)


# Plot Y versus X
# %matplotlib inline
if do_plot:
    plt.figure()
    plt.plot(geod_neu_xy.x_m, geod_neu_xy.y_m, '.', color='gray', alpha=0.3, label=label_geod)
    plt.plot(xyz.x_m, xyz.y_m, '.', color='tab:purple', alpha=0.3, label=label_xyz)
    plt.plot(filtd.x_m, filtd.y_m, '-', color='tab:blue', alpha=0.5, label=label_iterp)
    plt.ylim(ax_lim(geod_neu_xy.y_m,3)) # n is multiple of std dev
    plt.xlabel('Metres')
    plt.ylabel('Metres')
    plt.title('%s Y - X' %args.site)
    plt.legend()
    plt.savefig('%s_yx.png' %output_L2_base, dpi=300)

# Plot Z versus X
if do_plot:
    plt.figure()
    plt.plot(geod_neu_xy.x_m, geod_neu_xy.z_m, '.', color='gray', alpha=0.3, label=label_geod)
    plt.plot(xyz.x_m, xyz.z_m, '.', color='tab:purple', alpha=0.3, label=label_xyz)
    plt.plot(filtd.x_m, filtd.z_m, '-', color='tab:blue', alpha=0.5, label=label_iterp)
    plt.ylim(ax_lim(geod_neu_xy.z_m,3)) # n is multiple of std dev
    plt.xlabel('Metres')
    plt.ylabel('Metres')
    plt.title('%s Z - X' %args.site)
    plt.legend()
    plt.savefig('%s_zx.png' %output_L2_base, dpi=300)

# +
# 3D Plot ZYX
x = filtd.x_m
y = filtd.y_m
z = filtd.z_m
s = 3
t_num = mdates.date2num(filtd.index.to_pydatetime())
norm = plt.Normalize(vmin=t_num.min(), vmax=t_num.max())
cmap = plt.cm.viridis

views = [
    dict(elev=45, azim=65),
    dict(elev=5, azim=90),
    dict(elev=75, azim=90),
]

fig = plt.figure(figsize=(15, 6))
fig.subplots_adjust(right=1.15)
axes = []
scatters = []
for i, v in enumerate(views, start=1):
    ax = fig.add_subplot(1, 3, i, projection='3d')
    sc = ax.scatter(x, y, z, c=t_num, s=s, cmap=cmap, norm=norm, alpha=0.25, edgecolor='none')
    ax.set_xlabel('X (m)')
    ax.set_ylabel('Y (m)')
    ax.set_zlabel('Z (m)')
    ax.set_ylim(ax_lim(y, 5))
    ax.invert_xaxis()
    ax.invert_yaxis()
    ax.view_init(elev=v['elev'], azim=v['azim'])
    axes.append(ax)
    scatters.append(sc)
    ax.tick_params(labelsize=10)

sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
sm.set_array([])
cbar = fig.colorbar(sm, ax=axes, pad=0.02, shrink=0.7, location='right')
cbar.formatter = FuncFormatter(lambda val, loc: mdates.num2date(val).strftime('%Y-%m-%d'))
cbar.update_ticks()

plt.tight_layout()
plt.savefig('%s_zyx.png' %output_L2_base, dpi=300, bbox_inches='tight', pad_inches=0.2)
# -

# Plot Time versus X
if do_plot:
    plt.figure()
    if os.path.exists(exclusions_file):
        excl = pd.read_csv(exclusions_file, parse_dates=['excl_start', 'excl_end'])
        for ix, row in excl.iterrows():
            plt.axvspan(row.excl_start, row.excl_end, alpha=0.1, color='tab:red')
    plt.plot(geod_neu_xy.index, geod_neu_xy.x_m, '.', color='gray', alpha=0.3, label=label_geod)
    plt.plot(xyz[xyz.interpolated==False].index, xyz[xyz.interpolated==False].x_m, '.', color='tab:purple', alpha=0.3, label=label_xyz)
    plt.plot(filtd_disp.index, filtd_disp.x_m, '-', color='tab:blue', alpha=0.5, label=label_iterp)
    plt.ylabel('Metres')
    plt.title('%s X - Time' %args.site)
    plt.legend()
    plt.savefig('%s_xt.png' %output_L2_base, dpi=300)

# Plot Y versus Time
if do_plot:
    plt.figure()
    if os.path.exists(exclusions_file):
        excl = pd.read_csv(exclusions_file, parse_dates=['excl_start', 'excl_end'])
        for ix, row in excl.iterrows():
            plt.axvspan(row.excl_start, row.excl_end, alpha=0.1, color='tab:red')
    plt.plot(geod_neu_xy.index, geod_neu_xy.y_m, '.', color='gray', alpha=0.3, label=label_geod)
    plt.plot(xyz[xyz.interpolated==False].index, xyz[xyz.interpolated==False].y_m, '.', color='tab:purple', alpha=0.3, label=label_xyz)
    plt.plot(filtd_disp.index, filtd_disp.y_m, '-', color='tab:blue', alpha=0.5, label=label_iterp)
    plt.ylim(ax_lim(geod_neu_xy.y_m,3)) # n is multiple of std dev
    plt.ylabel('Metres')
    plt.title('%s Y - Time' %args.site) 
    plt.legend()
    plt.savefig('%s_yt.png' %output_L2_base, dpi=300)

# Plot Time versus Z
if do_plot:
    plt.figure()
    if os.path.exists(exclusions_file):
        excl = pd.read_csv(exclusions_file, parse_dates=['excl_start', 'excl_end'])
        for ix, row in excl.iterrows():
            plt.axvspan(row.excl_start, row.excl_end, alpha=0.1, color='tab:red')
    plt.plot(geod_neu_xy.index, geod_neu_xy.z_m, '.', color='gray', alpha=0.3, label=label_geod)
    plt.plot(xyz[xyz.interpolated==False].index, xyz[xyz.interpolated==False].z_m, '.', color='tab:purple', alpha=0.3, label=label_xyz)
    plt.plot(filtd_disp.index, filtd_disp.z_m, '-', color='tab:blue', alpha=0.5, label=label_iterp)
    plt.ylim(ax_lim(geod_neu_xy.z_m,3)) # n is multiple of std dev
    plt.ylabel('Metres')
    plt.title('%s Z - Time' %args.site) 
    plt.legend()
    plt.savefig('%s_zt.png' %output_L2_base, dpi=300)

# #### Velocities (all)

# Plot 24-hour differenced velocities
if do_plot and args.legacy:
    plt.figure()    
    plt.errorbar(v24h_reg.index+pd.Timedelta(hours=12), v24h_reg.v_myr, yerr=v24h_reg.v_uncertainty_myr, elinewidth=1.4, ecolor='tab:blue', drawstyle='steps-mid', alpha=0.5)
    plt.errorbar(v5d_reg.index+pd.Timedelta(hours=(5*24)/2), v5d_reg.v_myr, yerr=v5d_reg.v_uncertainty_myr, elinewidth=1, color='k', ecolor='k', drawstyle='steps-mid', linewidth=2)
    plt.errorbar(v15d_reg.index+pd.Timedelta(hours=(15*24)/2), v15d_reg.v_myr, yerr=v15d_reg.v_uncertainty_myr, elinewidth=1, color='tab:red', ecolor='tab:red', drawstyle='steps-mid', linewidth=2)
    plt.ylim(ax_lim(v24h_reg.v_myr,10)) # n is multiple of std dev
    plt.title('%s 24-H, 5-D and 15-D velocity (legacy differencing)' %args.site)
    plt.ylabel('m/yr')
    plt.savefig('%s_v24h_5d_legacy.png' %output_L2_base, dpi=300)


# +
def epoch_plotting(v_ts, data_kwargs=None, error_kwargs=None):
    # Plot stepped velocities
    plt.plot(v_ts.index, v_ts['v_myr'], drawstyle='steps-post', **data_kwargs)
    # Now plot uncertainties. Note that we cannot use the errorbar call to also plot the velocities,
    # because the steps don't begin in the right places when called with irregularly spaced mid-point timestamps.
    # This is therefore a different situation to that of regularly spaced velocities and uncertainties.
    mid_pt_timestamps = (v_ts.index+np.abs(v_ts.index.diff(-1)/2))
    df_plotting = pd.DataFrame({'v_myr':v_ts.v_myr, 'v_unc':v_ts.v_uncertainty_myr})
    df_plotting.index = mid_pt_timestamps
    df_plotting = df_plotting[df_plotting.index.notnull()]
    df_plotting = df_plotting.dropna()
    plt.errorbar(df_plotting.index, df_plotting['v_myr'], linestyle='none',
                yerr=df_plotting['v_unc'], **error_kwargs)
    
    
if do_plot:
    plt.figure()
    epoch_plotting(v24h_epoch, 
                    data_kwargs=dict(color='tab:blue'),
                    error_kwargs=dict(elinewidth=1, ecolor='tab:blue', capsize=0, alpha=0.5))
    epoch_plotting(v5d_epoch, 
                    data_kwargs=dict(color='k', linewidth=2),
                    error_kwargs=dict(elinewidth=1, ecolor='k', capsize=1))
    epoch_plotting(v15d_epoch, 
                    data_kwargs=dict(color='tab:red', linewidth=2),
                    error_kwargs=dict(elinewidth=1, ecolor='tab:red', capsize=1))
    plt.ylim(ax_lim(v24h_reg.v_myr,10)) # n is multiple of std dev
    plt.title('%s 24-H, 5-D and 15-D velocity (observational epoch differencing)' %args.site)
    plt.ylabel('m/yr')
    plt.savefig('%s_v24h_5d_epochs.png' %output_L2_base, dpi=300)
# -

# #### Velocities (summer only)

# Summer-only plots
# Legacy velocities
if do_plot and args.legacy:
    ystart = v24h_reg.index.year.unique().min()
    yend = v24h_reg.index.year.unique().max()
    years = [ystart] if ystart == yend else range(ystart, yend)
    for year in years:
        v24h_here = v24h_reg.loc[f'{year}-05-01':f'{year}-10-01']
        print(year, v24h_here.obs_cover_percent.mean())
        if v24h_here.obs_cover_percent.mean() <= 0.1:
            continue
        
        plt.figure()
        v5d_here = v5d_reg.loc[f'{year}-05-01':f'{year}-10-01']
        v15d_here = v15d_reg.loc[f'{year}-05-01':f'{year}-10-01']
        plt.errorbar(v24h_here.index+pd.Timedelta(hours=12), v24h_here.v_myr, yerr=v24h_here.v_uncertainty_myr, elinewidth=0.2, ecolor='tab:blue', drawstyle='steps-mid', alpha=0.5)
        plt.errorbar(v5d_here.index+pd.Timedelta(hours=(5*24)/2), v5d_here.v_myr, yerr=v5d_here.v_uncertainty_myr, elinewidth=1, color='k', ecolor='k', drawstyle='steps-mid', linewidth=2)
        plt.errorbar(v15d_here.index+pd.Timedelta(hours=(15*24)/2), v15d_here.v_myr, yerr=v15d_here.v_uncertainty_myr, elinewidth=1, color='tab:red', ecolor='tab:red', drawstyle='steps-mid', linewidth=2)
        plt.xlim(pd.Timestamp(year, 5, 1), pd.Timestamp(year, 10, 1))
        plt.ylim(ax_lim(v24h_reg.v_myr,10)) # n is multiple of std dev
        plt.title('%s 24-H, 5-D and 15-D velocity, Summer %s (legacy differencing)' %(args.site, year))
        plt.ylabel('m/yr')
        plt.savefig('%s_v24h_5d_summer_%s_legacy.png' %(output_L2_base, year), dpi=300)

# Summer-only plots
# Epoch-to-epoch velocities
if do_plot:
    ystart = v24h_epoch.index.year.unique().min()
    yend = v24h_epoch.index.year.unique().max()
    years = [ystart] if ystart == yend else range(ystart, yend)
    for year in years:
        plt.figure()
        v24h_here = v24h_epoch.loc[f'{year}-05-01':f'{year}-10-01']
        v5d_here = v5d_epoch.loc[f'{year}-05-01':f'{year}-10-01']
        v15d_here = v15d_epoch.loc[f'{year}-05-01':f'{year}-10-01']

        epoch_plotting(v24h_here, 
                data_kwargs=dict(color='tab:blue', alpha=0.5),
                error_kwargs=dict(elinewidth=1, ecolor='tab:blue', capsize=0, alpha=0.5))
        epoch_plotting(v5d_here, 
                        data_kwargs=dict(color='k', linewidth=2),
                        error_kwargs=dict(elinewidth=1, ecolor='k', capsize=1))
        epoch_plotting(v15d_here, 
                        data_kwargs=dict(color='tab:red', linewidth=2),
                        error_kwargs=dict(elinewidth=1, ecolor='tab:red', capsize=1))
        plt.xlim(pd.Timestamp(year, 5, 1), pd.Timestamp(year, 10, 1))
        plt.ylim(ax_lim(v24h_reg.v_myr,10)) # n is multiple of std dev
        plt.title('%s 24-H, 5-D and 15-D velocity, Summer %s (observational epoch differencing)' %(args.site, year))
        plt.ylabel('m/yr')
        plt.savefig('%s_v24h_5d_summer_%s_epochs.png' %(output_L2_base, year), dpi=300)
